Stanford university.Yeah. Hello everyone. Welcome to CS two, thirty one, am I songan? Today I'm gonna give a guest lecture on the efficient methods and hardware for deep learning. So I'm a fifth year PhD candidate here at Stanford, advised by professor Bill daly.

So in this course we have seen a lot of comm neural networks, recurring neural networks or even last time the are reinforcement learning, um, they are spending a lot of applications, for example, the self driving car, machine translation of a go and smart robots.

It is changing our lives, but there is a recent trend that in order to achieve such high accuracy, the models are getting a larger and larger. For example, for image net recognition, the winner from 2012 to 2015 the model size increased by sixteen X.

And just in one year for Baidu's deep speech, just in one year by training operations, the number of training operations increased by ten X.

So such large model creates lots of problems. Um, for example, the model size becomes larger and larger, so it's difficult for them to be deployed either on those. For example, from the mobile phones, if the item is a larger than a hundred megabytes, you cannot download until you connect to WiFi.

So those product managers, for example, Baidu, Facebook, they are very sensitive to the size of the binary size of their model.

And also, for example, the self driving car, you can want to do those on over the air update for the model, if the model is too large, is also difficult.

And the second challenge for those, um, point for those large models is that the training speed is extremely slow. For example, the resident one fifty two, which is only a few less than 1% actually more accurate than resident than one one takes one point half weeks to train on four am.

Maxwell forty and forty G P U for example, which greatly limits the ah, either we are doing homework or the researchers design new models is getting pretty slow.

And the third challenge for those bulky model is the energy efficiency.For example, you know, the AlphaGo beating lease, although last year took 2000 C P U's and 300 G P us, which cost 3000 dollars just to pay for the electric Bill, which is insane.

So either on those embedded devices, those models are draining your battery power or on the data center and increases the total cost of ownership of maintaining a large data center.

For example, Google are in their blog. They mentioned if all all the users using the Google voice search for just three minutes, they have to double their data center.

So that's a large cost.So reducing such cost is very, ah, important, and let's see where is actually the energy consumed.

The large model means lots of memory access you have to access load those models from the memory means more energy.

If you look at, um, how much energy is consumed by loading the memory versus, um, how much is consumed by multiplications and add those arithmetic operations, um, the memory access is more than.

Two, two or three orders of magnitude more energy consuming than those arithmetic operations.So how to make deep learning more efficient? So we have to improve energy efficiency by this algorithm and hardware Co design.

So this is the previous way we design hardware. For example, we have some benchmark say back two thousand six, two thousand six, and then run those benchmark and tune your C P C P architectures for those ah, benchmarks.

Now what we should do is to open up the box to see what can we do from the organism side first and see what is the optimal question Mark processing unit. But bridge breaks the boundary between the hours and hardware.

Between algorithm hardware to improve the overall efficiency.So today, ah, today's talk, I'm going to have the following agenda. We are going to cover four aspects, the however, in inference and training. So they form a small two by two matrix.

So, um includes the algorithm for efficient inference hardware for efficient inference and the algorithm for efficient training. And lastly, the hardware for efficient training. For example, I'm gonna cover the T P U, I'm gonna cover the vol, but before, before I cover those things, let's have a three slice for hardware one one.

A brief introduction of the families of hardware in such a tree.So in general, we can have roughly two branches. One is general purpose hardware.

It can, can, can, can do any only um applications versus the specialized hardware which is tuned for a specific kind of application or domain of applications.

So the general purpose hardware includes the C P U or the G P G P U, and the difference is that C P U is latency oriented single threaded. It's like a big elephant.

Well, the G P U is is throughput oriented. It has many small weak threats, but there are thousands of such small weak course, like a group of small ants, but there are so many ants.

And specialized hardware roughly. Ah, there are F P G A and asics. So F P G stand for field. Ah, programmable gate array, so it is programmable.

Hardware programmable. Those logic can be changed so it's cheaper for, um, for you to try new ideas and prototype.

But it's less efficient, it in the middle between the general purpose and pure as so as and for application specific integrated circuit. It has a fixed logic just designed for certain application. For example, deep learning and Google's T P U is a kind of asic.

And the neural networks we train on, the ebitda gpus is here.Another slide for hardware, one one is the number representations.

So in this slide I'm gonna, um, convey you the idea that all the numbers in computer are not represented by a real number. It's not a real number, but they are actually discrete even for those floating point you 32 bit floating point numbers. Ah, the resolution is, is not perfect, it's not, ah, it's not continuous, but it's discrete.

So for example, F P 32 meaning using 32 bit to represent the floating point number. So there are three components in the representation. The sign, bit exponent, exponent bit, the manta and the number it represent is is shown by minus one to the s times one point m.

Times two to the exponent.So similarly, there is A P 1616 using sixteen bit to represent fourteen point number.

Um, so I in particular I'm gonna introduce into eight where the Google T P U use using a integer to represent a fixed point number. So we have a certain number of bits for the integer followed by a redix point different from different layers, and lastly the fractional bits.

So why do we prefer those eight bit or sixteen bit rather than those traditionally the 32 bit floating point?

That's the cost. So I generated figure from a forty five, nine met technology about the energy cost versus the area cost for different operations in particular. Let's see here.

Going from 32 bit to sixteen bit, we have about four times four times reduction in energy and also about four times reduction in the area.

Area means money. Every millimeter square takes money to take out a chip.So it's very credit, it's very beneficial for hardware design to go from 32 bit to sixteen bit.

That's why you hear about the am media from past architecture later starting to support R P sixteen. That's the reason why it's so, so beneficial.

For example, previous battery class four hours, now it become sixteen hours, but what it means to reduce the energy cost by ah four times, but here still there's a problem of.

Large energy costs for reading the memory. And let's see, how can we deal with this memory reference so expensive? How do we deal with this problem later?

So let's switch here and come to our topic directly. So let's first introduce algorithm for efficient inference.

So I'm going to cover six topics. This is a really long slide, so I'm going to go relatively fast.So the first idea I'm going to talk about is pruning, pruning the neural networks.

For example, this is original neural network, so what I'm trying to do is can we remove some of the weights, remove some of the weights and still have the same accuracy.

It's like pruning a tree. Get rid of those redundant connections.This is first proposed by professor young Kong back in 1989 and I revisited this problem in 26 years later on those modern deep neural nets to see how it works.

So not all parameters are useful actually. For example, in this case you can want to fit a single line, but you are using a quadratic term. Apparently the 0.01 is redundant parameter.

So I'm gonna train the connect first and then prove some some of the connections and train the remaining ways and do this process iteratively and as a result, I can reduce the number of connections in Alex land from 60 million parameters to only 6 million parameters, which is 1010 times less the computation.

So.So this is the accuracy. So the y access is how much parameters do prune away and the y access is the accuracy you have, right?

So we want to have less parameters, but we also want to have the same accuracy before. We don't want to sacrifice accuracy.

For example, at 80% and 80% we not zero away 80% of the parameters, but accuracy dropped by 4%.That's intolerable.

But the good thing is that if we retrain the remaining weights between the remaining weights, the accuracy can fully recover here.

And if we do this process iteratively by pruning and retraining, pruning and retraining, we can fully recover the accuracy, not until we are prune away 90% of the parameters.

So if you go back, go back to home and try on your ipad on notebook, just zero away 50% of the parameters in one of your homework, you will astonly find that the accuracy actually doesn't hurt.

So we just mention convolution neural net. How about Ireland and our t M? So I tried with this neural talk. Again, pruning away 90% of the weights doesn't hurt the blue score.

And here are some visualizations for example, the original picture ah, the neural talk says a basketball player in the what uniform is playing with the ball versus crooning away 90% and says a basketball player in a white uniform is playing with a basketball.

And on and so on. But if you are too aggressive, right, if you are too aggressive, say you prune away 95%95% of the weights, the network is gonna get drunk. It says a man in a red shirt and white and black shirt is running through a field.

So there is really a limit of threshold you have to take care of during the pruning.So interestingly, after I did the work, did some literature research and find actually the same, the same pruning procedure actually happen to human brain as well.

So when we were born, there are about fifty trillion synapses in the brain.And at one year old, this number served to a thousand trillion, and as we become adolescent it becomes smaller, actually fifty trillion fifty five hundred trillion in the end, according to the study by nature.

So this is very interesting.And also, the prunian changed the way distribution, because we are removing those small connections and after we retrain them, that's why it becomes solved in the end.

Yeah. So the question is how do we deal with those zero, zero connections? So we force them to be zero in all the dations.

Pleasure.Yeah, so very simple, small weight drop. It sorted people small.So, um, the next idea with sharing. So now we have remember our ah end. The goal is to re-remove ah connections and so that we can have less memory footprint so that we can have more energy efficient deployment. Right now we have less number of parameters by pruning. We we wanna have is wanna have less number of bits per parameter. So they multiply together they get a small model, right.

So the idea is like this.Not all numbers, not all the weights has to be the exact number. For example, two point zero nine two point one two.

Or all these four weights, you just put them.Using two to represent them. That's enough. Otherwise too accurate. Just numbers. Just leads to over fitting.

So the idea is I can class the weight if they are similar, just using a centroid to represent the number instead of using the full precision weight.

So that every time I do the inference, I just do inference on this single number.For example, this a way, this is four by four weight matrix. Ah, in a certain layer, right. And what I'm gonna do is do k mean classroom by having the similar weights, sharing the same centroid for example, two point oh nine, two point one, two, I store an index of three pointing to here.

Um, so that the good thing is we need to only store the two bit index rather than the 32 bit. Fourteen point number. That's sixteen times saving.

Sixteen times saving.And how do we train such neural network?They have, they are binded together, right? So after we get the gradient, we color them and the same pattern as the weight. And then we do a group by operation by having all the net weights with the same index grouped together, and then we do a reduction by summing them up.

Then, multiplied by the learning rate subtracted from the original centroid, that's one iteration of thegd for such weight shared neural network.

So remember, previously after pruning, this is what the distribution like.And after, after um with sharing, they become discrete. There are only sixteen different values here, meaning we can use four bits, four bits to represent each number.

And by training on such weight shared neural network right training on such new share with shared neural network, these ways can adjust, you know.

It is the subtle changes that compensate for the loss of accuracy.So let's see, this is the number of bits we give it. This is accuracy for convolution layers for convolution layers, not until four bits, not until four bits does the accuracy begin to drop and for those fully connected layers, very astonishingly two bits, not until two bits, only four numbers that the accuracy begins to drop and this result is per layer.

So we have covered two methods, right pruning and weight sharing. How, if, what if we combine these two methods together, do they work well? So ah, by combining these methods, this is compression ratio of the smaller better and the left. And this is the accuracy. We can combine it together and make the model about three times 3%3% of its original size without hurting the accuracy at all.

Compared with each working individually, by 10% the accuracy begins to drop.When compared with the cheap svd method is have a better compression ratio.

And the final idea is we can apply those halfman coding to use more number of bits for those infrequent numbers, infrequent appearing weights and less number of bits, for those are more frequently appearing weights.

So by combining these three methods, pruning with sharing and also huffman coding, we can comprise the neural networks set of neural networks.

Ranging from ten X to 49 X ten X 49 X and without without hurting the prediction accuracy, sometimes a little bit better, but maybe that is noise.

So the next question is, am these models are just pre train models by, say Google Microsoft? Can we make a compact model, a pump compact model to begin with even before, even before such compression?

So, squeezenet, you may have already worked with this neural network model in a homework.So that's, um, the idea is we are having a, um, squeeze layer here to shield the three by three convolution with fewer number of channels with fewer number of channels. So that's where squeeze come from. And here we have a two branches rather than four branches in, as in the exception model.

So, ah, as a result, the model is extremely compact. It doesn't have any. Ah, fully connected layers. Everything is fully convolutional the last layer, the last layer, is a global global p.

So what if we apply the deep compression algorithm on such already compact model? Will it be getting even smaller?

Okay, so this is Alex net after compression. This is squeeze net even before compression. It's fifty X smaller fifty X, smaller than Alex net, but the same accuracy.

After compression510 X 510 X smaller but the same accuracy, only less than half a megabyte, less than half a megabyte.

This means it's very easy to fit such small model on the ah on the cash on the ah, all three cash, which is usually tens of michaelytes I.

So what does it mean? It's possible to achieve speed up.So this is the speed up I am measured for these fully connected layers, only for now fully connected layers only for now on the C P U G P U and mobile G P U before pruning and after pruning the weight.

And on average, I observe the three X speed up on the C P U, ah, about three X speed up on the G P U and roughly five X speed up on the mobile G P U, which is A T T K one.

And so is the energy efficiency. Um, in an average improvement from three X to six, X on the C P U G P and mobile G P U.

And these ideas are used in these companies.Okay, um, having talk about Pro sharing, which is a not linear quantization method, and going to talk about quantization, which is widely used, widely used in the T P U design, how the T P U design is using only eight bit or eight bit for inference. And the way how they can use that is because of the quantization and let's see how that they work.

So, um, quantization has, this is complicated figure, but the intuition is very simple.You run the neural network and train it with the normal floating point numbers with floating point numbers.

Um, and quantize the weight and activations that gather the statistics for each layer. For example, what is the maximum number, maximum number and how many bits are enough to represent this dynamic range? Then you use that number of bits for ah, for the integer part and the rest of the eight bit, or seven bit for the ah for the other part of the eight bit representation.

And also we can fine tune, fine tune in the floating point format, or we can also use feed forward with fixed point and back propagation with update. With the floating point number, there are lots of different ideas to have better accuracy.

And this is the result for how many number of the number of bits versus what is the accuracy? For example, using fixed eight bit, the accuracy for Google net doesn't drop significantly and for B G G, sixteen, it also remains pretty well for the accuracy.

Well, if you're going down to six bit, the accuracy begins to drop pretty dramatically.Okay, next idea, low rank approximation.

Low rank approximation.It turn out that for a convolution layer for a convolution layer, you can break it. You can break it into two convolution layers, one convolution here, followed by a one by one convolution.

So that it's like you break a complicated problem into two separate small problems.This is for convolution layer. I see as we can see, achieving about two X speed up there is almost no loss of accuracy.

And achieving the speed up of five X roughly 6% loss of accuracy.And this also works for fully connected layers. The simplest idea is using svd.

Ah, to break it into one matrix into two, matrix is and follow this idea, um, we can. Ah, this paper proposes to use the tensor train to break down one fully connected layer into a train. Lots of fully connected layers. That's why it's called train.

Okay.So even going even more crazy, can we use only two bits? Ah, two, two weight, or three ways to represent a neural network, a turn weight or binary weight?

Ah, we already seen this distribution before after pruning right? There are some positive ways. Like, wait, can we just use three numbers? Just use one minus one zero to represent a neural network.

And this is our recent paper clear that we maintain a full precision weight during training time during training time. But at inference time we only keep the scaling factor and the turnary weight. So during inference we only need three weights, only need three weights. That's very efficient.

And making the model very small.And this is the proportion of the positive zero and negative weights they can change during the training, so is their absolute value.

And this is the visualization, the visualization of the kernels by this train turn quantization ah, we can see some of them are maybe a corner detector like here.

And also here, some of them are maybe edge detector, for example, this filter.Some of them are corner detector, like here, this filter.

Actually, we don't need such fine green resolution. Just three ways are enough.So this is the validation accuracy on image net with the Alex net.

I like that, so the bas line is the baseline accuracy with floating point 32 and the red line is our result.

Pretty much the same accuracy converged.Compared with the full precision weights.Okay, last idea. We know grad transformation.

So this is about how do we implement deep neural net, how do we implement the convolutions?This is the conventional direct convolution implementation method. Ah, the slight credit to Julian, a friend from media. So originally we just do the, ah, element wise, do a dot product for those nine elements in the future and nine elements in the Mid image and then sum it up.

So, for example, for every output we need nine X C, number of multiplication and ads.Am we know great convolution? We know great convolution is another ah method, equivalent method. Equivalent method is not lost is equivalent method.

Propose the first in this paper fast algorithm for convolution neural networks that instead of directly doing the convolution moving one by one at first, transforms, transforms the input feature map.

To another feature map which contains only the way only one zero point five, two that can be efficiently implemented with shift and also transform the future into a four by phone, four by four tensor.

So what we are going to do here is some see and do a element wise, element wise product. So there are only 1616 multiplications happening here and then we do inverse, transform together four outputs.

So, the transform and inverse transform can be amortized and the multiplication of whether there can be ignored. So in order to get four output we need nine times channel times four, which is 36 times channel multiplications originally for the direct convolution, but now we need sixteen times c.

For four output, sixteen times number of channels for output, so that is 2.225 X 2.25 X less number of multiplications to perform the exact same multiplication.

And here is the speed up 2.25 X, so theoretically theoretically 2.25 X speed up and in real um, so from q five, ah, they incorporated such win grad convolution algorithm am. This is on the V G net. I believe the speed up is roughly 1.7 to two X speed up pretty significant.

And after coion five, the kian begins to use the winograd convolution algorithm.Okay, so so far we have covered those efficient algorithms for efficient inference.

We covered pruning with sharing quantization and also a window grade. Ah, binary turnary. Um, so now let's see what is the optimal hardware? Let's see what is the optimal hardware for those efficient inference and what, for example, what is a Google T P U.

So there are a wide range of specific domain specific architectures, or asics, for deep neural networks they have. A common goal is to minimize the memory access, minimize the memory access to save the power.

For example, the areas from MIT by using the road stationary data flow to minimize the off off cheap dram access and target now from China academy of science ah offered all the weights on on cheap dram instead of having to go to off cheap dram of cheap dram. So am the T P U from Google is using eight bit integer eight bit integer to represent the numbers and stan I proposed the e architecture that support those compressed and sparse deep neural network inference.

So this is what the tpu looks like.Um, it's actually smartly, ah, can be put into the disk, into the disk, disk drive. Ah, up to four cars per server.

And this is the high level architecture for the ah, for the Google T P U am. Don't be overwhelmed am, it's actually the kernel part here is this matrix multiplication unit.

Ah, so it's a two fifty six by two fifty six matrix multiplication unit, so one in one single cycle in one single cycle, it can perform sixty four, four k, six, four kilo, those number of multiplication and accumulate operations.

So running at am seven, seven hundred megahertz, the throughput, the throughput is 92 parallel blocks per second.

Op per second, because it's actually into the operation.So, which is about 25 X as G P U and a hundred more than hundred X C P U.

I noticed T P has a really large software, um.Managed on buffer.It is 24 megabytes24 megabytes.The C P U the memory for the C P U of the cash for the C P the three cash is usually sixteen megabytes. This is 24 megabytes, which is pretty large.

And is powered by two D D R three dram channels.So this is a little weak because the bandwidth is only thirty gigabytes per second, thirty gigabytes per second compared with the most recent G P U than H B M nine hundred nine hundred gigabytes per second. But that is back in.

A D D R four is released in 2014 so that makes sense because the design is a little during that day. Ah, use D D R three, but if you using D D R four or even hand high band withth memory, the performance can be even boosted.

Ah, so this is a comparison about Google's T P U compared with the C P U G P U ah ah this K A G P U by the way and the T P U.

Um.So the area is pretty much smaller, like half the size of the C P and G P U, and the power consumption power consumption is roughly 75 watts. Um, let me see this number the peak.

Ter op per second is much higher than the cpn. Gp is about 9090 ter ops per second, which is pretty high.

Ah, so here is a workload. Ah, thanks to ah David sharing the slide am. This is the workload at Google am. They did the benchmark on these T P u's. So it's a little interesting that a convolution neural net convolution neural net only account for 5%5% of data center workload. Most of them is a multi layer perception. Those fully connected layers about 61% maybe.

For ads, I'm not sure, and about 29%29% of the workload in data center is the long short term memory. For example, speech recognition or machine translation, I suspect.

Um, yeah.So in order, remember just now we have seen there are ninety ter ops per second, but what actually number of ter ops per second can be achieved.

So this is a basic tool to measure the bottleneck of a computer system.Whether you are banene by the arithmetic or you are botanene by the memory bandwidth.

It's like if you have a bucket, the lowest, the lowest bar of the bucket determines how much water you can hold in the bucket. So in this region you are bottlenecked bottlenecked by the memory bandwidth.

So, ah, the ax access is the arithmetic intensity, arithmetic intensity, which is number of floating point.

Operations per bite, the ratio between the computation and the memory of memory.Band with overhead. So the why access is the actual, the actual attainable ah peak. Ah, attainable performance. Here is the peak performance. For example, the peak performance when you do a lot of operation after you fetch a single piece of data. If you can do a lot of re-operation on top of it, then you are bottlenecked by the arithmetic.

But every after you fetch a lot of data, a lot of data from the memory, but you just do a tiny little bit, tiny little bit of arithmetic. Then you will be bottlenecked by the ah, memory bandwidth. So how much you can can fetch from the memory depend ah determines how much, um, real performance you can get. And remember, ah, there is a ritual when it is one here, ah, this, this region. It happens to be the same as the turning point.

Is the actual memory bandwidth of your system?So let's see what is it like for the T P U? The T P U peak performance is really high, about 9090 ops per second.

For those convolution nets convolution nets, they are pretty much saturated saturated the.Ah, the, the peak performance.

But there are lots of neural networks that has a utilization less than 10% utilization of less than 10% meaning that ninety t ops per second is actually achieved.

You actually achieve about three to twelve t ops per second in real case.But why is it like that? Um, the reason is in order to have those real time, real time guarantee, like the user not wait for too long, you cannot batch, you cannot batch a lot of users images or speech voice data at the same time, so as a result, for those fully connected players.

They have very little reuse, have very little reuse, so ah, they are bottleneck by the memory footprint memory bandw with for those convolution units, for example, this one, this blue one that achieved eighty six eighty six which is seen in zero.

Let's see, c and zero. The ratio between the ops and the number of memory is the highest right?Is pretty high 2000 more than more than 2000. Compared with um, other multi layer perception or long short term memory, the ratio is pretty low.

So this figure compares the this is the T P U and this one is the C P U, this is the T P U.Am here is the memory bandwidth, the peak memory bandwidth and the ratio of one here. So G P U has the highest memory bandwidth and here is where are these neural networks lie on this curve. So the asterisk is for the tpu it's still higher than other dots.

But if you not comfortable with this log scale, this log scale figure, this is what is what is like. Put it in linear scale, the line roof line. Um, so pretty much everything disappeared except for the T P U results.

Am so still all these lines are, although there are higher than the C P and G P U is still way below the theoretical way below the theoretical ah, peak bandwidth, a peak floating point, a peak operations per second.

So as I mentioned before, it is really bottlenecked by the low latency requirement so that you can have a large batch size, so that's why you have low operations per bike.

And how do you solve this problem? You wanna have a less number of memory footprint so that you can, ah, reduce the memory bandwidth requirement.

So you need to one solution is to compress the model.And the challenge is how do we build a hardware? How do we build a hardware that can do inference directly on the compressed model?

So I'm gonna to introduce my design of E I, the efficient inference engine, which, ah, deals with those sparse and compress the model to save the memory bandwidth.

And the rule of thumb, like we mentioned before, is taking advantage of sparsity first anything, times zero is and zero. So don't store it. Don't compute on it.

And second idea is this kind of, you don't need that much of full precision, but you can approximate it.

So by taking advantage of the sparsely with, we get about, um, ten X saving in the computation, five less memory footprint, the two X difference is due to the index overhead.

And by taking advantage of the sparse activation, meaning after riddle, if anything, if activation is zero, then ignore it. You save another three X for computation.

Embed by such weight sharing mechanism, you can use four bits, four bits to represent each weight rather than 32 bit that's another eight X saving in the memory footprint.

Um, so this is physically log how the weight are stored, a matrix by four by eight matrix and this is how physically they are stored. Only the nine zero weight, only the nine zero weight are stored.

So you don't need to store those zeros. You save the bandidth fetching those zeros and also you, you, I, using the relative index to further save. Ah, the, the number of memory, number of memory overhead.

So in the computation like this figure shows, we are running the multiplication only on nine zero, nine zero, then skip it.

Only broadcast to the nine zero waits.And if it is zero, skip it.If it's at nine, zero.To the multiplication in another cycle to the multiplication. So the idea is anything multiplied by zero is zero.

So this is a little complicated. I'm going to go very quickly. I'm going to have a lookup table that decode the four bit weight into the sixteen bit weight and using the four bit relative index pass through address accumulator to get the sixteen bit absolute index.

And this is what the hardware architecture like in the high level, you can feel free to refer to my paper for detail.

Okay, speed up.So using such efficient hardware architecture and also model compression, um, this is the original pay result we have seen for C P U T P R mobile Gu now e is here.

One eighty nine times faster than the cpu, and about thirteen times faster than the G P U.So this is the energy efficiency on the log scale is about 24,000 X more energy efficient than C P and about 3000 X more energy efficient than gpio.

It means, for example, previously if your battery can last for one hour, now it can last for 3000 hours, for example.

So if you say I think is always better than C P U and G P U because you know it customize hardware. So this is comparing E I E with the peer asics. For example g and two north.

It has a better throughput.Better energy efficiency by an order of magnitude.Compared with other asics, not to mention the cpu, gpu and igs.

Okay, so we have covered half half of the journey. We mentioned inference. We pretty much covered everything for inference. Now we are going to switch gear and talk about training. How do we train neural networks efficiently?

How do we train it faster? So again, we are starting with algorithm. First, efficient algorithms followed by the hardware for efficient training.

Ah, so for efficient training algorithms, I'm going mention four topics. The first one is parallelization and then mix of precision training.

Um, which is just released about one month ago and at media G T C, so fresh knowledge, um, and then model distillation followed by my work on bencharance training or better regularization technique.

Okay, so let's start with paralization.So this figure shows, ah, very hard work. Community must be very familiar with this figure show. So as time goes by, what is the trend for? The number of transistors is keeping increasing, but the single threaded single threaded performance is getting plateaued in recent years.

And also, the frequency is getting in recent years.Because of the power constraint, the stop of not skating.

An interesting thing is the number, of course, the number, of course, is increasing.So what we really need to do is parallelalyization how do we paralyze the problem to take advantage of parallel processing?

Actually, there are a lot of opportunities for parallelism in deep neural networks.For example, we can do a data parallel, for example, feeding two images, feeding two images into the same model and run them at the same time.

This doesn't affect the latency for a single input. It doesn't make it shorter. But, um, it makes batch size larger. Basically, if you have a am three, four machines, your effective batch size becomes four times as before.

So it requires the coordinated weight update.For example, am this is paper from ah Google am, there is a param server. There is a param server as a master and couple of slaves running their own piece of data of training data and update the gradient to the parameter server and get the updated weight for them individually.

That's how a data parallelism is handled.Another idea is there could be a model parallelism. You can split your model and handle it to different processors or different threads.

For example, this is the image, this is the image you want to run convolution on this image that is six, six dimension for loop.

Ah, what you can do is you can. You can cut the input image by two by two blocks so that each thread or each processor handles one fourth of the image. Although there is a small halo here in between, you have to take care of.

And also you can paralyze by the output or input feature map.And for those fully connected layers, how do we paralyze the model? It's even simpler. You can cut. You can cut the model into half and handle it to different threads.

And the third idea, you can even do hyper parameter parallel. For example, you can tune your learning rate, your weight decay on a different machines for those cause green parallelism, so there are so many alternatives you have to tune.

Okay, um, small summary of the parallelism. So there are lots of parallelism in deep neural networks. For example, the data parallel parallelism. You can run multiple training images, but you cannot have limited, ah, a limited number of threats. Our processors, because you were limited by webs size. If it's too large, stochastic grid in descent becomes good in descent. That's not good.

And you can also run the model parallelism. Ah, split the model either by cutting the image or cutting the ah, cutting the weight commolution weight. Ah, maybe just now give a small um, oh yeah. So never mind either cutting the ah image or cutting the um fully connect layers. So it's very easy to get a sixteen to 64 G P us training. One model in parallel having very good speed up, almost linear speed up.

Okay, next interesting thing, mixed precision with fp sixteen or fp 32.So remember in the beginning of this lecture I have a chart showing the energy and area overhead for F P sixteen bit versus 32 bit.

Going from 32 bit to sixteen bit, you save about four X, four X the energy and four X the area, right?

So can we train a deep neural network with such low precision with floating point sixteen bit rather than 32 bit?

It turned out.We can do that partially.Partially by partially. I mean, we need fp 32 in some places.And where are those places?

So we can do the multiplication in.3016 bit as input.Um, and then we have to do the submission submission in 32 bit accumulation and then convert the result to 32 bit to store the weight. So that's where the mixed precision come from.

So for example, we have a master weight stored in floating point 32 we converted down, converted to floating point sixteen, and then we do the feed forward feed forward with sixteen bit weight, sixteen bit activations. We get a sixteen bit activation here in the end.

When we are doing back propagation, all the computation is also done with floating point sixteen b.Am very interesting here for the weight, for the weight, we get a floating point sixteen bit.

Gradient here for the weight sixteen bit gradient for the weight, but when we are doing the update, so w.

Times learning plus learning rate times the gradient. That operation has to be done in thirty two thirty two.

That's where the mixed precision coming from. You see there are two colors, which here is sixteen bit. Here is the 32 bit. That's where the mixed precision come from.

So does such low precision sacrifice your prediction accuracy for, for your model? So this is the figure from ah from Nvidia. I just released a couple of weeks ago actually. Um, thanks to.

Paul is giving me the slide.The convergence between floating point 32 versus the vot tens up, which is basic mix of precision training, are actually pretty much the same convergence. If you do a little bit, they're pretty much the same.

And for reset, the mixed precision sometimes behave a little better than the full precision weight, maybe because of noise.

But in the end, after you train the model ah, this is result for Alex net inception v three and res fifty with F P 32 versus the fp six mixed the precision training, the accuracy is pretty much the same.

For these two methods, a little bit worse, but not by too much.Okay, so having talked about the mixed precision training, the next idea is to train with model distillation.

Um, for example, you can have multiple neural networks, Google net, Mid net resnet for example. And the question is can we take advantage? Can we take advantage of this different models? Of course, we can do model onmble right? Can we utilize them as teacher, utilize them as teacher to teach a small junior neural network to have it perform as good as the senior neural network.

So this is the idea. So you have multiple large, powerful junior senior neural networks to teach this student model and hopefully it can get better results.

And the idea they do that is.Instead of using this hard label, for example, for call dog cat, the probability for dog is a hundred percent, but the the output of the geometric ensemble of those large teacher neural networks. Maybe the dog has 90%90% and the the car the cat is about 10%.

And the magic happens here.You wanna have a softened softened result label here for example, the dog is 30%, the cat is 20% still the the dog is higher than the cat. So the the prediction is still correct, but it use this soften soft label. Use this soft label to train the the student neural network rather than use this very hard label to train the student neural network.

And mathematically, you control how much do you make it soft by this temperature?Are during the soft Max, controlling by this temperature.

And the result is that starting with a true model that classifies eighty eight, fifty, fifty eight point nine percent of the test spr correctly, the new model converges to the 57% only trend on 3%3% of the data.

So that's the magic for model distillation using this kind of soft label.And the last idea is my recent paper using a better regularization to train deep neural nets.

We have seen these two figures before. We prove the neural network having less number of ways, but have the same accuracy. Now what I did is to recover and retrain those ways showing red and make everything turn altogether.

To increase the model capacity after it is trained at a low dimensional space.It's like you learn the trunk first and then gradually learn all those lives and learn everything together.

It turns out image that data set, it performs relatively about one one percent to 4% absolute improvement of accuracy.

And also general purpose works on on short term memory and also recurring neural n collaborated with Baidu.

So I also open source this better trend model are on the D S D model zoo where there are trained all these models, Google net V G G res net and also squeeze net and also Alex net. So if you are interested, feel free to check out these models.

And compared with the cafe model.Here are some examples on thanks for dance training helps, helps with the image caption.

Ah, for example. This is a very, very challenging figure. The original baseline of neural talk as a boy in a red shirt is climbing a rock wall.

Ah, and the sparse models as a younger jumping off a tree probably mistaken the here with either the rock or the tree. But that sparse training by using this kind of regularization on a low dimensional space, it says a young girl in a pink shirt is swing on a swing.

And there are a lot of examples am due to the limit of time, I will not go over them one by one, for example, group of people are standing in front of a building, there's no building. A group of people are walking in a park.

Feel free to check out the paper and see more interesting results.Okay, finally, we come to hardware for efficient training.

Ah, how do we take advantage of the algorithms we just mentioned? For example, parallelism? Mixed precision? Ah, how are the hardware designed to to actually take advantage of such features we can obtain?

First gpus.Um, this is the immediate past call G P U G P 100 which was released last year.So it supports twenty tar flops on F P sixteen.

It has sixteen gigabytes of high boundaries memory. It's seven hundred fifty seven hundred fifty gigabytes per second.

So remember, computation and memory bandwidth are the two factors determines your overall performance. Whichever is lower, you cannot. You will suffer.

So this is really high band with seven hundred seven hundred gigabytes compared with D D R three is just ten or thirty gigabytes per per second.

Consumes seven, three, three hundred watts.And?It's done in sixteen nanometer process and have a 160 gigabytes per second. And we so remember we have computation, we have memory, and third third thing is, is the communication. All three factors has to be balanced in order to achieve a good performance.

So this is very powerful, but the even more exciting, even more exciting.Just about a month ago, Jensen released the newest architecture called vol the vol gus.

And let's see what is inside the G P, just released less than a month ago.So it has.Fifteen p 32 tar flops and.

What is new here? There is a hundred and twenty tensor t specifically designed for deep learning and will later cover what is the tensor core and what is this a hundred and twenty ah, coming from.

And rather than 750 gigabytes per second this year am the H B M two, they using 900 big gigabytes per second memory band, very exciting and twelve neter process has a die size of more than 800 millimeters square, really large chip.

And supported by 300 gigabytes per second and we link.So, ah, what's new in the most interesting thing? For, for us, for deep learning, is this this thing called tensor core?

So, tensor, what is a tensor core? Tensor core is a unit is actually an instruction, an instruction that can do the four by four matrix times of four by four matrix ah theused F I may stand forused multipation and add in ah this mix precision operation just in one single clock cycle, one clock cycle.

So let's decipher a little bit. What does this mean? So a mix precision is exactly as we mentioned in the ah, last chapter. So we are having ah, A P sixteen for the multiplication, but forumulation accumulation, we are doing it with F P 32 that's where the mixed precision come from.

So you are, let's see, how many operations? Four by four by four, it's sixty four, sixty four multiplication than just in one single cycle.

That's twelve X increase in the speed up of the vol or compared with the past call, which is released just last year.

Ah, so this is the um, result for matrix multiplication on different sizes. The ah speed up of pastal of over over Pascal is roughly three X three X faster during these matrix multiplications.

And what we care more is not only matrix multiplication, but actually running the deep neural nets.So both for training and for inference.

And for training a rather than fifty, by taking advantage of this tensor core in this v 100 it is two point four two point four X faster than the p 100 using A P 32.

So on the right hand side, it compares, compares, the inference, the inference speed up given. Given a seven microsecond latency requirement, what is the number of images per second it can process? It is a measurement of throughput.

Again, the um um test of the v 100 v 100 over p 100 by taking advantage of the tensor course is three point seven three point seven faster than the p 100.

So this figure gives roughly an idea what is the tensor core, what is the integer unit, what is the floating point unit?

So.This, this whole figure, this whole figure is ah, this am is a single as m stream market processor. So SM is partitioned into four processing blocks. One, two, three, four, right.

And in each block, there are eight fp 64 cores here.And 1632 and sixteen in 32 course here, units here.

And then there are two of the new missed precision pencil cores specifically specifically designed for deep learning.

And also there are the warp scheduler, dispatch unit and register file.As before.So what is new here is the tenser call unit here.

So here is a figure comparing the recent generations of n media g P's from kepler.To Maxwell, to Pascal, to b.

We can see everything is keeping improving. For example, the boost clock has been increased from about 800 megahertz to 1.4 gigahertz.

And?From devota generation there begins to have the tensor core, um, ah units here, which has never existed before.

And before the m the Maxwell, the G P U are using the G D D R five G D D R G D D R five.And then after the pastasc call, G P U um, the H P M begins to came into place the high bandwidth memory750 gigabytes per second here900 gigabytes per second compared with D D R three thirty gigabytes per second.

And the memory size actually didn't increase by too much memory size didn't increase by too much, and the power consumption is actually also remaining roughly the same.

By giving the increase of the computation, you can fit them in the fixed power envelope, but still an exciting thing.

And the manufacturing process is also improving from the twenty nanometer, twenty nanometer, sixteen nanometer, all the way to twelve nanometer.

And the ship areas are also increasing to 800 millimeters' really huge.So, ah, you may interested in the ah comparison of the G P U was the T P U right? So how do they compare with each other? Am so in the original T P U paper um T P actually designed roughly in the year of 2015.

And this is comparison of the past call p forty gpus designing release in 2016.Um, so T P the power consumption is lower, um, as larger on cheap memory of 24 mighty. Really large on cheap as managed by the software.

And then both of them support int eight operations.Well, the inference per second, given a ten nanometer latency, the comparison for T P S one X for the p forty is about two X.

So just last week, just last week in the Google I O.A new nuclear bomb is landed on the earth that is the Google cloud T P U, so now T P U not only support an inference but also support training.

So there's very limited, ah, information we can get beyond this Google blog. So they are cloud T P U delivers up to a hundred and eighty pyouopes a hundred eighty peroulo to train and run machine learning models.

And this is multiple ah ah t cloud T P U making it into A T P U pod T P pod, which is build with sixteen, um, the second generation T P U and delivers up to 11.5 pet flops of machine learning acceleration.

So in the Google blog, ah, they mentioned that one of the large scale translation models, Google translation models, used to take a full day, a full day to train on this 32 ah, best commercially available G P U, probably p forty or p 100 maybe. And now it trains the same accuracy, accuracy just after one afternoon with just one eighth, one eighth of ATP pod, which is pretty exciting.

Okay, so it's a little wrap up. We covered a lot of stuff we mentioned in the four dimension space of the algorithm and hardware inference and training recovered the algorithms for inference. For example pruning quantization window grade convolution, binary ter weight sharing for example, and then the hardware for efficient inference. For example, the the T P U the asic T P U um that take advantage of int eight in teacher eight um.

And also the, ah, my design of E I accelerator that take advantage of the sparsity anything multiplied by zero is zero, so don't store it, don't compute on it.

And also the efficient algorithm for training. For example, how do, how do we do parallelization and the most recent research on how do we use mixed precision training by taking advantage of F P sixteen rather than F P 32 to do training, which is four times, four times saving in energy and four times saving in the area, which doesn't quite sacrifice the accuracy you get from the um, for from the training.

And also that spark training using, um, better regularization regularization and also the um am teacher student model, right? You have a multiple teacher new network and have a small student network that you can distill the knowledge from the teach teacher new network by, ah, by a temperature.

And finally we covered the hardware for efficient training and introduced ah, two nuclear bombs. One is the vol G P U, the other is the ah, the the T P U version to the cloud T P U um, and also the amazing um tensor tensor tensor course am in the ah, newest generation of n media. G P U.

And we also review the progression of a wide range, the recent ah, immediate G P us from the kepler k forty, that actually when I started my research, what we used in the beginning all the way to and then k forty and forty.

And then Pascal. And then finally, the exciting volta gpu so every year there is a nuclear bomb in the spring.

Okay, ah, little over um look ahead in the future. So in the future of the city we can imagine there are lot of U A I applications used in say smart society, smart care, ah I O devices, smart retail for example, Amazon go and also smart home, um, lots of scenarios and it poses a lot of challenges on the hardware design that requires the low latency, privacy, mobility and energy efficiency. You don't want your battery to drink very quickly.

So it's both challenging and very exciting era for the ah Co design for the both the machine learning, deep new network model architectures and also the hardware architecture.

So we have moved from PC era to mobile era. Now we are in the AI first era and hope you are as excited as I am for this kind of brain inspired cognitive computing research.

And thank you for your attention. I'm glad to take questions.We have five minutes.Of course.Yes, some of the ideas are pretty good. I think this opportunity.

No.Question is, what can we do to make the hardware better?Alright, ah, the question is how do we, ah, challenge the challenges and more opportunity for those small embedded devices to run deep new network or in general the AI algorithms? Yeah. So those are them I discussed in the beginning about inference.

Here.These are the techniques that can enable such am inference or AI running on embedded devices by having less number of ways, fewer base per weight and so quantization low rank approximation, the small matrix, same accuracy, even going to b or turnary weight, having just two bits um, to, to do the computation rather than sixteen or even 32 bit. And also the win transformation.

Those are also the enabling algorithms for those low power embedded devices.Okay, the question is if it's binary weight, um, the software developers may be not able to take advantage, take advantage of it.

There's a way to take advantage of binary way of paper in extraordinary night. So in one register in one register there are 32 bit 32 bit. Now you can think of it as a 32 way. Parallelism, each bit is a single operation. So say previously you have ten of per second, now you get a thirty two hundred, three hundred and thirty.

Ops per second, you can do this bit wise am operation. For example, ignore operation, so one register file one one operation becomes 32 operation.

So you can, um, there's a paper, um, called X, or the very, ah, amazing implemented on the raspberry pie using this feature. Ah, to do real time detection. Very cool stuff.

Yeah, so, um, the trade offs is always so. The power area and performance in general, all the hardware design have to take into account the performance, the power and also the area.

Um, when machine learning comes, there's a fourth. There's a fourth figure of merit, which is the accuracy, what is accuracy?

And there is a fifth one which is program ability. So how general is your hardware? For example, if Google just want to use that for ah, AI and deep learning, am it totally fine that we can have a fully very specialized architecture just for deeper learning to support convolution motivated perception? Long, long short term memory. But G P us, ah, you know you want also want have support for those, you know, um.

Scientific computing or graphics? AR and VR. So that's, that's a difference first of all.Ah, T P U basically is a basic right. It's a very fixed function but you can still program it with those course instructions am so people ah from ah Google smart design, those course granularity instruction. For example, one instruction just load the matrix story matrix doutions do myrix multipation those course green instructions am and they have a software software managed memory.

Also called scratch pad is different from cash where you know the determines where to evict something from the cash but now since you know the computation pattern um there there's no um no no need to do out of order execution to do you know branch prediction. No such things. Everything is determined so you can take amount of of it and and maintain a fully um software managed scratch pad to ah, remove, ah reduce the data movement.

And remember, data movement is the key for reducing the memory footprint and energy consumption.So, yeah.

And nirvana architectures, actually, I'm not quite familiar, didn't those lines?Come in a little bit later now.

Oh yeah, of course. Those algorithms can certainly be applied to low power embedded devices if your interest can show you a.

Um.Some examples of OS.Where is that?Of my previous projects running deep neural netets on for example on the John this is T K one ah mobile G P U to do real time tracking and detection. This is me playing filled by by a DJ I John to do the detection tracking.

And also this is F P G A, um, doing the, ah, deep neural networks, pretty small, just this, this large doing the face alignment and detecting the eyes, the nose and the mouth.

At a pretty high frame rate.Consuming only three watts, only three watts. Ah, this is a project I did at Facebook doing the, ah, deep neural nets on the mobile phone to do image classification, for example, says as a laptop.

Or you can, ah, feed it with the image and says it's a selfie has person face, etcetera. So there's lots of opportunity for those embedded or mobile deployment of deep neural nets.

I know there's a team doing that but I cannot comment too much. Probably there's a team of stuff.Okay, thanks everyone. If you have any questions, feel free to drop me an email.