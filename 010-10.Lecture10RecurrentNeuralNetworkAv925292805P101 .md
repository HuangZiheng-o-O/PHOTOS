Stanford university. Okay, can everyone hear me?OK, eh, sorry for the delay. I had a bit of technical difficulty. Today was the first time I was trying to use my new touch bar macbook Pro for presenting, and none of the adapters are working, so I had to switch laptops at the last minute, so thanks.

Eh, sorry about that. So today is lecture ten. We're talking about recurrent neural networks, so as as usual, a couple administrative notes, so.

Uh, we're, we're working hard on assignment one, grading EM. Those grades will probably be out sometime later today. EM, hopefully they can get out before the a two deadline. That's what I'm hoping for. On a related note, assignment two is due today at 1159 PM. So who who's done with that already?

About half you guys. So I I remember I did warn you when the assignment went out that it was quite long and to start early. So you were warned about that, but hopefully you guys have some late days left.

EM also as another reminder the midterm will be in class on tuesday. EM if you kind of look around the lecture hall there are not enough seats in this room to see all the enrolled students in the class so we'll actually be having the midterm in several other lecture halls across campus and we'll be sending out some more details on exactly where to go in the next couple of days. So a bit of another bit of announcement. We've been working on this sort of fun bit of extra credit thing for you to play with that we're calling the training game. So this is this cool browser based experience where you can go in and interactively train neural networks and tweak the hyperparameters during training EM and this should be a really cool interactive way for you to practice some of these hyperpareter tuning skills that we've been talking about. The last couple of lectures EM so this is not required but eh this I think will be a really useful experience to gain a little bit more intuition into how some of these hyper parameters work for different types of data sets in practice EM so we're still eh working on getting all the bugs worked out of this. Set up EM and we'll probably send out some more instructions on exactly how this will work in the next couple of days EM but again not.

Required, but please do check it out. I think it'll be really fun and a really cool thing for you to play with EM and we'll give you a bit of extra credit if you do some if you if you end up working with this and doing a couple of runs of it. So we'll again send out some more details about this soon once we get all the all the bugs worked out.

So as a reminder, last time we were talking about CNN architectures, EM, we kind of walked through the timeline of some of the various winners of the imagenet classification challenge. Kind of the breakthrough result, as we saw was the alexnet architecture in 2012 which was a nine layer convolutional network. It did amazingly well. And it sort of kickst started this whole deep learning revolution in computer vision and kind of brought a lot of these models into the mainstream.

EM, then we skip ahead a couple years and saw that in 2014 imagenet challenge we had these two really interesting models, vgg and Google net, which were much deeper EM. So vgg was they had a sixteen and a nineteen layer model and Google net was I believe a 22 layer model EM. Although one thing that is kind of interesting about these models is that eh, the 2014 imagenet challenge was right before batch normalization was invented. So at this time before the invention of batch normalization, training these relatively deep models of roughly twenty layers was very challenging. So in fact both of these two models had to resort to a little bit of hackery in order to get their deep models to converge.

So EM for vgg they have these sixteen and nineteen layer models, but actually they first trained an eleven layer, an eleven layer model because that was what they could get to converge, and then added some extra random layers in the middle and then continued training to actually train these sixteen and nineteen layer models. So managing this training process was very challenging in 2014 before the invention of batch normalization.

EM, similarly for Google net, we saw that Google net had these auxiliary classifiers that were stuck into lower layers of the network EM, and these were not really needed for the class to get good classification performance. This was just sort of a way to, to cause extra gradient to be injected directly into the lower layers of the network. And this sort of this again was before the invention of batch normalization. And now once you have these networks with batch normalization, then you no longer need some of these slightly ugly hacks in order to get these deeper models to converge.

EM then we also saw in 2015 in the 2015 imageet challenge was this really cool model called resnet, these residual networks that now have these shortcut connections that actually have these little residual blocks where we're going to take our input, pass it through the residual blocks and then add that output of the, of the then add our input to the block to the output from these convolutional layers. EM, this is kind of a funny architecture, EM, but it actually has two really nice properties. One is that if we just set all the weights in this residual block to zero, then this block is computing the identity. So in some way EM, it's, it's relatively easy for this model to learn not to use the layers that it doesn't need.

EM, in addition, it kind of adds this interpretation to l two regularization in the context of these neural networks because once you put l2 regularization remember on your, on the weights of your network, that's going to drive all the parameters toward zero. And in maybe your standard convolutional architecture is driving toward zero, maybe doesn't make sense. But in the context of a residual network, if you drive all the parameters towards zero, that's kind of encouraging the model to not use layers that it doesn't need because it will just drive those, the residual blocks towards the identity whether not needed for classification.

EM, the other really useful property of these residual networks has to do with the gradient flow in the backward pass. If you remember what happens at these addition gates in the backward pass when upstream gradient is coming in through an addition gate, then it will split and fork along these two different paths. So then when upstream gradient gradient comes in one, it'll take one path through these convolutional blocks, but it will also have a direct connection of the gradient through this residual connection. So then when you look at when you imagine stacking many of these residual blocks up on top of each other and our network ends up with hundreds of potentially hundreds of layers, then these residual connections give a sort of gradient superhighway for gradients to flow backward through the entire network. And this allows it to train much, much more, much easier and much faster and actually allows these things to converge reasonably well even when the model is potentially hundreds of layers deep.

And this idea of managing gradient flow in your models is actually super, super important everywhere in machine learning and super prevalent in recurrent networks as well. So we'll definitely revisit this idea of gradient flow later in today's lecture.

So then, eh, we kind of also saw a couple other more exotic, more recent eh CNN architectures last time, including dense net and fractional net. And once you think about these architectures in terms of gradient flow, they make a little bit more sense EM. These things like dense net and fractal net are adding these additional shortcut or identity connections inside the model. And if you think about what happens in the backward paths for these models, these additional funny topologies are basically providing direct paths for gradients to flow from the loss at the end of the network more easily into all the different layers of the network. So I think that, again, this idea of managing gradient flow properly in your CNN architectures is something that we've really seen a lot more in the last couple of years, and we'll probably see more moving forward as more exotic architectures are invented.

We also saw this kind of nice plot plotting EM performance of like the number of flops versus the number of parameters versus the runtime of these various models. And there's some interesting characteristics that you can dive in and see from this plot. EM one idea is that vgg and alexnet have a huge number of parameters and these parameters actually come almost entirely from the fully connected layers of the models. So alexnet has something like roughly 62 million parameters. And if you look at that last fully connected layer, the final fully connected layer in alexnet is going from an activation volume of six by six by two fifty six into this fully connected vector of 496. So if you imagine what the weight matrix needs to look like at that layer, that weight matrix is gigantic. Its number of entries is six by six, six times six times two fifty six times four ninety six and if you multiply that out you see that that single layer has 38 million parameters. So more than half of the parameters of the entire alexnet model are just sitting in that last fully connected layer. And if you add up all the parameters in the in just the fully connected layers of alexnet including these.

Other final fully connected layers, you see something like 59 of the 62 million parameters in alexnet are sitting in these fully connected layers so that then when we move to other architectures like Google net and resnet, they do away with a lot of these large, fully connected layers in favor of global average pooling at the end of the network. And this allows these networks to really these, these, these nicer architectures to really cut down the parameter count in in these architectures.

So, eh, that, that was kind of our brief recap of the CNN architectures that we saw last lecture. And then today we're going to move to one of my favorite topics to talk about, which is recurrent neural networks.

So, so far in this class, we've seen what I like to think of as kind of a vanilla feed forward network. All of our network architectures have this flavor where we receive some input and that input is a fixed size object, like an image or a vector EM. That input is Fed through some set of hidden layers and produces a single output, like a classification, like a set of classification scores over a set of categories.

EM, but in in some contexts in machine learning we want to have more flexibility in the types of data that our models can process. So once we move to this idea of recurrent neural networks, we we have a lot more opportunities to play around with the types of input and output data that our networks can handle. So once we have recurrent neural neural networks we can do what we call these one to many models where where maybe our input is some object of fixed size, like an image, but now our output is a sequence of variable length such as a caption EM, where different captions might have different numbers of words. So our output needs to be variable in length.

EM, we also might have many to one models where our input could be variably sized. This might be something like a piece of text and we want to say what is the sentiment of that text, whether it's a positive or negative in sentiment or in a computer vision context, you might imagine taking as input a video and that video might have a variable number of frames. And now we want to read this entire video of potentially variable length and then at the end make a classification decision about maybe what kind of activity or action is going on in that video.

EM, we also have, we might also have problems where we want both the input and the output to be variable in length. EM, we might see something like this in machine translation where our input is some maybe sentence in English which could have a variable length and our output is maybe some sentence in French which also could have a variable length. And crucially the the length of the English sentence might be different from the length of the French sentence. So we need some models that have the capacity to accept both variable length sequences on the input and on the output.

Finally, we might, we might also consider problems where our input is variable length, like something like a video sequence with a variable number of frames, and now we want to make a decision for each element of that input sequence. So in the context of videos, that might be making some classification decision along every frame of the video. And recurrent neural networks are this kind of general paradigm for handling variably sized sequence data that allow us to pretty naturally capture all of these different types of of setups in our models.

So recurrent neural networks are actually important even even for some problems that have a fixed size input and a fixed size output. Recurrent neural networks can still be pretty useful. So in this example we might want to do, for example, sequential processing of our input. So here we're receiving a fixed size input like an image and we want to make a classification decision about like what number is being shown in this image. But now rather than just doing a single feed forward pass and making the decision all at once, this network is actually looking around the image and taking various glimpses of different parts of the image. And then after making a some series of glimpses then it makes its final decision as to what kind of number is present. So here we had here, even though our input and outputs, our input was an image and our output was a classification decision. Even in this context, this idea of being able to handle variably length processing with recurrent neural networks can lead to some really interesting types of models.

EM this we, there's a really cool paper that I like EM that applied the same type of idea to generating new images where now we want, we want the model to synthesize brand new images that look kind of like the images it saw in training. And we can use a recurrent neural network architecture to actually paint these output images sort of one piece at a time in the output. So you can see that even though our output is this fixed sized image, we can have these models that are working over time to compute parts of the output one at a time sequentially, and we can use recurrent neural networks for that type of setup as well.

So EM, after this this sort of cool pitch about all these cool things that r and n can do, you might wonder what exactly are these things? So in general a recurrent neural network EM is, is this little has this little recurrent core cell and it will take some input X, feed that input into the R N N and that R N N has some internal hidden state EM and that internal hidden state will be updated every time that the rnn reads, reads a new input EM and that hidden that internal hidden state will be then Fed back to the model the next time it reads an input.

And frequently, we will want our r and n to also produce some output at every time step. So we'll have this pattern where it will read an input, update its hidden state and then produce an output.

EM. So then the question is, what is the functional form of this recurrence relation that we're computing? So inside this little green r and n block, we're computing some recurrence relation with a function f, so this function f will depend on some weights w it will accept the previous hidden state ht minus one as well as the input at the current state X T, and this will output the next hidden state or the updated hidden state that we call ht.

EM and now then then as we read the next input, this hidden state, this new hidden state ht will then just be passed into the same function as we read the next input X T plus one, and now if we wanted to produce some output at every time step of this network, we we might attach some additional, some additional fully connected layers that, um, read in this ht at every time step and make that decision based on the hidden state at every time step.

And one thing to note is that we use this same function F W and these same weights w at every time step of the computation.

So then eh then kind of the simplest functional form that you can imagine is what we call this vanilla recurrent neural network. So here we we have the same functional form from the previous slide where we're taking in our previous hidden state and our current input. And we need to produce the next hidden state. And the kind of simplest thing you might imagine is that we have some weight matrix W X H that we multiply against the input X T as well as another weight matrix W H H that we multiply against the previous hidden state. So we make these two multiplications against our two states, add them together and squash them to it through a tan h so we get some kind of nonlinearity in the system.

EM, you might be wondering why we use a ten h here and not some other type of non linearity. After all that. We've said negative about ten h in previous lectures and I think we'll return a little bit to that later on when we talk about more advanced architectures like the lsstm.

So then this. So then in addition, in this architecture, if we wanted to produce some Y T at every time step, you might have another weight matrix w, you might have another weight matrix that accepts this hidden state and then transforms it to some y to produce maybe some class score predictions at every time step.

EM, and when I think about recurrent neural networks, I kind of think about EM. You can also you can kind of think recurrent neural networks in two ways. One is this concept of having a hidden state that feeds back at itself recurrently but I find that picture a little bit confusing and sometimes I find it clear to think about unrolling this, this computational graph for multiple time steps. And this makes the data flow of the hidden states and the inputs and the outputs and the weights maybe a little bit more clear.

So then at the first time step we'll have some initial hidden state h zero. This is usually initialized to zeroos for most context in most contexts. And then we'll have some input X T, this initial hidden state h zero and our current input X T will go into our F W function. This will produce our next, our next hidden state h one.

And then we'll repeat this process when we receive the next input. So now our current h one and our X one will go into that same F W to produce our next output h two.

And this process can repeat, we repeat, over and over again as we consume all of the input X ts in our sequence of inputs.

And now one thing to note is that we can actually make this even more explicit and write the w matrix in our computational graph. And here you can see that we're reusing this same w matrix at every time step of the computation. So now every time we have this little F W block, it's receiving a unique h and a unique X, but all of these blocks are taking the same w.

And if you remember, we talked about how gradient flows in back propagation when you reuse the same when you reuse the same node multiple times in a computational graph. Then remember during the backwards pass you end up summing the gradients into the w matrix when you're computing d lost D W, so if you kind of think about the back propagation for this for this model, then you'll have a separate gradient for w flowing from each of those time steps. And then the final gradient for w will be the sum of all of those individual per time step gradients.

EM, we can also write this uh this yt explicitly in this computational graph. So then this uh, this output ht at every time step might feed into some other little neural network that can produce A Y T, which might be some class scores or something like that at every time step.

We can also make the loss more explicit. So in many cases you might imagine producing. You might imagine that you have some ground truth label at every time step of your sequence, and then you'll compute some loss, some individual loss at every Tim step of the of of these output y ts. And this loss might, it will frequently be something like a softmax loss in the case where you have maybe a ground truth label at every time step of the sequence.

And now the final loss for the entire for this entire training step will be the sum of these individual losses. So now we had a scalar loss at every time step and we just sum them up to get our final scalar loss at the top of the network. And now if you think about again, back propagation through this thing we need in order to train the model, we need to compute the gradient of the loss with respect to w, so we'll have loss flowing from that final loss into each of these time steps. And then each of those time steps will compute a local gradient on the weights w, which will all then be summed to give us our final gradient for the weights w.

Now, if we have sort of this many to one situation where maybe we want to do something like sentiment analysis, then we would, we would typically make that decision based on the final hidden state of this network, because this final hidden state kind of summarizes all of the context from the entire sequence.

EM also, if we have kind of a one to many situation where we want to receive a fixed size input and then produce a variably sized output, then one, then you'll commonly use that fixed size input to initialize somehow the initial hidden state of the model. And now the recurrent network will tick for each cell in the output and now as you produce your variably sized output, you'll unroll the graph for each each element in the output.

So this then then when we talk about these sequence to sequence models where you might do something like machine translation, where you take a variably sized input and a variably sized output, you can think of this as a combination of the many to one, plus a one to many. So we'll kind of proceed in two stages, what we call an encoder and a decoder. So here the encoder will receive the variably sized input, which might be your sentence in English, and then summarize that entire sentence using the final hidden state of the encoder network.

And now, now we're now in, now we're in this many to one situation where we've summarized this entire variably sized input in this single vector and now we have a second decoder network which is a one to many situation which will input that single, that single vector summarizing the input sentence and now produce this variably sized output which might be your sentence in another in another language. And now in this variably sized output we might make some predictions at every time step maybe about what word to use. And you can imagine kind of training this entire thing by unrolling this computational graph, summing the losses at the output sequence and just performing back propagation as usual.

So as a bit of a concrete example, one thing that we frequently use recurrent neural networks for is this problem called language modeling. So in the language modeling problem, we want to read some sequence of, we want to have our network sort of understand how to produce natural language. So in the so this, this might happen at the character level where our model will produce characters one at a time. This might also happen at the word level where our model will produce words one at a time.

But in a very simple example, EM, you can imagine this character level language model where we want, where the network will read some sequence of characters and then it needs to predict what will, what will the next character be in this stream of text. So in this, in this example, we have this very small vocabulary of four letters H E L and o and we have this training, this example training sequence of the word hello H E L L O.

So during training, when we're training this language model, EM, we will feed the characters of this of of this training sequence as inputs. As X ts to our input will feed the characters of our training sequence. These will be the X ts that we feed in as the inputs to our recurrent neural network.

EM and then each of these inputs, uh, it's a letter and we need to figure out a way to represent letters in our, in our network. So what we'll typically do is figure out what is what is our total vocabulary. In this case our vocabulary has four elements and each letter will be represented by a vector that has zeros in every slot but one and a one for the slot in the vocabulary corresponding to that letter. So in in this little example, since our vocab has the four letters H E L O, then our input sequence, the h is represented by a four element vector with a one in the first slot and zeroos in the other three slots. And we use the same sort of pattern to represent all the different letters in the input sequence.

So now eh during during this, during this forward pass of what this network is doing EM. At the first timest step it will receive the input letter h EM that will go into the first rnn to the rnn cell and then will produce this output yt which is the network making predictions about for each letter in the vocabulary. Which letter does it think is most likely going to come next? So in in this example, eh, the correct output letter was e because our training sequence was hello, but the model is actually predicting EM.

Eh, I think it's actually predicting o as the most likely letter. So in this case this prediction was wrong and we would use a soft Max loss to quantify our unhappiness with these predictions and now at the next time step EM we would feed in this second letter in the training sequence e EM and this this process will repeat. We'll now we we represent e as a vector EM use that input vector together with the previous hidden state to produce a new hidden state and now use the second hidden state to again make predictions over every letter in the vocabulary EM in this case because our training sequence was hello after the letter letter e, we want our model to predict l EM and in this case.

Uh, our model may had very low predictions for the letter l, so it would incur high loss. And you can, and you kind of repeat this, this process over and over. And if you train this model with many different sequences, then eventually it should learn how to predict the next character in a sequence based on the context of all the previous characters that it've seen before.

And now if you think about what happens at test time after we train this model, one thing that we might want to do with it is as a sample from the model and actually use this trained neural network model to synthesize new text that kind of looks similar in spirit to the text that it was trained on. So the way that this will work is we'll typically seed the model with some input prefix of text. In this case the prefix is is just this single letter h, and now we'll feed that letter h through through the first Tim step of our recurrent neural network. It will produce this distribution of scores over all over all the characters in the vocabulary. But now at training time we'll use these scores to actually sample from it. So we'll use a softmax function to convert those scores into a probability distribution and then we will sample from that probability distribution to eh actually synthesize the second letter in the sequence.

And in this case, even though the scores were pretty bad, maybe we got lucky and sampled the letter e from this probability distribution.

And now we'll take this letter e that was sampled from this distribution and feed it back as input to the network at the next time step. So now we'll take this e, pull it down from the top, feed it back into the network as one of these sort of one hot factorial representations, and then repeat the process in order to synthesize the second letter in the output.

EM, and we can repeat this process over and over again to synthesize a new sequence using this trained model, where we're synthesizing the sequence one character at a time, using these predicted probability distributions at each time step.

Question.Yeah, that's a, that's a great question. So the question is why? Why might we sample instead of just taking the character with the largest score?

So in this case, uh, because the probabilities distribution that we had, it was impossible to get the right character. So we had the sample. So the example could work out and it would make sense. But but in practice sometimes you'll see both. So sometimes you'll just take the arg maxs probability EM and that will sometimes be a little bit more stable. But one one advantage of sampling in general is that it lets you get diversity from your models. So EM sometimes you might have the same input, maybe the same prefix or in the case of image captioning maybe the same in image. But then if you sample rather than taking the arg Max, then you'll see that sometimes these train models actually are able to produce multiple different types of reasonable output sequences depending depending on which samples they take at the first time steps. So it's actually kind of a kind of a benefit because we can get now more diversity in our outputs.

Another question.But we feed in the soft Max vector instead of the one element vector you mean at test time.

Yeah, so the question is EM at test time could we feed in this whole softmax vector rather than a one hot vector? So there's kind of two problems with that. One is that that's very different from the data that it saw at training time. So in general, if you ask your model to do something at test time, which is different from training time than it'll usually blow up, it'll usually give you garbage and you'll usually be sad EM. The other problem is that in practice our vocabulary might be very large. So maybe in this simple example our vocabulary is only four elements so it's not a big problem. But if you're thinking about generating words one at a time, now your vocabulary is every word in English language which could be something like tens of thousands of elements. So in practice, EM this, this first element, this this first operation that's taking in this one hot vector EM is often performed using sparse vector operations rather than dense vectors. So it would be sort of computationally really bad if you wanted to have this load of 10,000 element softax specter.

So that's why we usually use a one hot instead, even at test time.So eh, this this idea that we have a sequence and we produce an output at every, at every time step of the sequence and then finally compute some loss. This is sometimes called back propagation through time because you're imagining that in the forward pass you're kind of stepping forward through time. And then during the backward pass you're sort of going backwards through time to compute all your gradients, EM. But this can actually be kind of problematic if you want to train with sequences that are very, very long. So if you imagine that we were kind of trying to train a neural network language model on maybe the entire text of Wikipedia, which is by the way, something that people do pretty pretty frequently, then this would be super slow. Then every time we made a gradient step, we would have to make a forward pass through the entire text of all of Wikipedia and then make a backward pass through all of Wikipedia and then make a single gradient update. And that would be super slow. Your model would never converge. It would also take a ridiculous amount of memory. So this would just be really bad.

So in practice, what people do is this sort of approximation called truncated back propagation through time. So here the idea is that even even though our input sequence is very, very long and even potentially infinite, what we'll do is that when we're, when we're training the model, we'll step forward for some number of steps. EM, maybe like a hundred is, is kind of a ballpark number that people frequently use. And we'll step forward for maybe a hundred steps, EM, compute a loss only over this, this subsequ sequence of the data and then back propagate through this subsequ sequence and now make a gradient step.

And now when we repeat now we still have these these hidden states that we comp computed from the first batch. And now when we compute this next batch of data we will carry those hidden those, those hidden states forward in time EM. So the forward pass will be exactly the same. But now when we compute a gradient step for this next batch of data, we will only back propagate again through this through this second batch. So now we'll make a gradient step based on this truncated back propagation through time.

And, and this process will continue where now when we make the next batch, we'll again copy these hidden states forward, but then step forward for some amount and then step backward only for some small number of time steps EM. So this is, you can kind of think of this as being analogous to stochastic gradient descent in the case of sequences. Remember when we talked about training our models on large dataas sets than these large dataas sets, it would be super expensive to compute the gradient over every element in the dataas sets. So instead we kind of take small, small mini batches instead and use mini batches of data to compute gradient steps in, in kind of the image classification case.

Question.Uh, is this kind of making the question is, is this kind of making the markov assumption? So no, not really because we're carrying this hidden state forward in time forever. So it's, it's making a markovian assumption in the sense that conditioned on the hidden state that the hidden state is all that we need to predict the entire future of, of, of the sequence. EM, but that assumption is kind of built into the recurrent neural network formalism from the start and that's not really particular to back propagation through time. EM back propagation through truncated back prop through time is just a way to approximate these gradients without going making a backward pass through your potentially very large sequence of data.

So EM this, this all sounds very complicated and and confusing and it sounds like a lot of code to write but in fact this can actually be pretty concise. So EM Andre has this example of what he calls min char r and n that does all of this stuff in just like 112 lines of python. So it handles building the vocabulary. It trains the model with truncated back propagation through time and then it can actually sample from that model in in actually not too much code. So even though this sounds like kind of a big scary process it's actually not too difficult. So I'd encourage you if you're confused to maybe go check this out and step through the code on your own time and see of all of these concrete steps happening in code, EM, so this is all in just a single file all using numpy with no dependencies. This is relatively easy to read.

So then once we have this idea of training a recurrent neural network language model, we can actually have a lot of fun with this. And we can take in sort of any text that we want, take in like whatever random text you can think of from the Internet, train our recurrent neural network language model on this text, and then generate new text. So in this example, EM, we took this entire text of all of Shakespeare's works and then used that to train a recurrent neural network language model on on all of Shakespeare.

And you can see that at the beginning of training, it's kind of producing maybe random gibberish garbage. But throughout the course of training, it ends up producing things that seem relatively reasonable. And after, after this model has been trained pretty well, then it produces text. That seems kind of Shakespeare askque to me, EM, why do what that day? Reply whatever. You can read this like it looks kind of like Shakespeare.

EM, and if you actually train this model even more and let it converge even further and then sample these sample even longer sequences, you can see that it learns all kinds of crazy cool stuff that really looks like a Shakespeare play. So it knows that that it uses maybe these, these headings to say who's speaking. Then it uses, it produces these bits of text that have crazy dialogue that sounds kind of Shakespeare askque it knows to put line breaks in between these different things. And this is all really cool. I'll just sort of learn from the structure of the data.

EM, we can actually get even crazier than this. So this was one of my favorite examples I found online. There's this. Is anyone a mathematician in this room? Has anyone taken an algebraic topology course by any chance?

Wow, a couple. That's impressive, EM. So, so you probably know more algebraic topology than me, but I found this open source algebraic topology textbook on online. So it's just a whole bunch of Tech files that are this super dense mathematics. And because Tech is of, this lets you write equations and diagrams and everything just using plain text. So we can actually train our recurrent neural network language model on the raw the text source code of this algebraic topology textbook.

And if we do that, then after we sample from the model, then we get something that seems like kind of like algebraic topology. EM, so it knows to put equations, it puts all kinds of crazy stuff. EM like to prove study. We see that f sub is a covering of X prime, blah, blah, blah, blah blah. It knows to put unions, it knows to put squares at the ends of proofs. It makes lemmas. It makes references to previous leas, right? Like we hear like. So it's namely by lemma question. We see that r is geometrically something. So it's actually pretty, pretty crazy. It also sometimes tries to make diagrams. So for those of you who've taken algebraic topology, you know that these commutative diagrams are kind of a thing that you work with a lot. So it kind of got the general gist of how to make those diagrams, but they actually don't make any sense.

And actually.One of my favorite examples here is that it sometimes omits proofs, so it'll sometimes say it, sometimes say something like theorem, blah, blah, blah, blah, blah, proof omitted. So this thing kind of has gotten the gist of how some of these math textbooks look like.

Eh, we can have a lot of fun with this. So we also tried training one of these models on the entire source code of the Linux kernel EM because again that's just this character level stuff that we can train on and then when we sample this it actually again looks like c source code so it knows how to write if statements. It has like pretty good code formatting skills and knows to indent after these if statements it knows to put curly braces. It actually even makes comments about some things that are usually nonsense. EM one one problem with this model is that EM it knows how to it knows how to declare variables but it doesn't always use the variables that it declares and sometimes it tries to use variables that haven't been declared. So this wouldn't compile. I would not recommend sending this as a pull request to Linux.

EM, this thing also figures out how to recite the g EM this gnu license character by character. So it kind of knows that it need to recite the Gu license and after the license comes from includes then some other includes then source code. So this thing has actually learned quite a lot about the general structure of the data where again during training all we asked this model to do was try to predict the next character in the sequence. We didn't tell it any of this structure, but somehow just through through the course of this training process, it learned a lot about the latent structure in this sequential data.

Yeah, so it knows how to write code, does a lot of cool stuff.So EM, I I had this paper with, with Andrea a couple of years ago where we trained a bunch of these models and then we wanted to try to poke into the brains of these models and figure out what are they doing and why are they working? So we saw in these recurrent neural networks have this hidden vector, which is maybe some vector that gets updated over every time step. And then what we wanted to try to figure out is could we find some elements of this vector that have some semantic interpretable meaning.

So what we did is we trained a neural network language model, one of these character level models on one of these data sets, and then we pick one of the elements in that hidden vector, and now we look at what is the value of that hidden vector over the course of a sequence.

EM to try to get some sense of maybe what these different hidden states are looking for. So when you do this, a lot of them end up looking kind of like random gibberish garbage. So here again, what we've done is we've picked one element of that vector and now we run this sequence forward through the trained model. And now the color of each character corresponds to the magnitude of that, that single scalar element of the hidden vector at every time step when it's reading the sequence. So you can see that a lot of the, a lot of the vectors in these hidden states are kind of not very interpretable. It seems like they're kind of doing some of this low level language modeling to figure out what characters should come next.

EM, but some of them end up quite nice. So here we found this vector that is looking for quotes. So you can see that there's this one hidden element, this one element in the vector that is off, off, off, off, off blue. And then once it hits a quote, it turns on and it remains on for the duration of of this quote. And now when we hit this second quotation Mark, then that cell turns off. So somehow, even though this model was only trained to predict the next character in a sequence, it somehow learned that a useful thing in order to do this might be to have some cell that's trying to detect quotes.

We also found this, this other cell that looks like it's counting the number of characters since a line break. So you can see that at the beginning of each line, this, this element starts off at zero. Throughout the course of the line, it gets gradually more reds that value increases, and then after the new line character, then it resets to zero. So you can kind of imagine that maybe this cell is letting the, the network keep track of when it needs to write to, to produce these new line characters.

We also found some that, eh, when we train on the Linux source code, we found some examples that are turning on inside the conditions of if statements. So this maybe allows the network to differentiate whether it's outside, outside and if statement or inside that condition, which might help it model these sequences better.

EM, we also found some that turn on in comments or some that seem like they're counting the number of indentation levels. So this is all just really cool stuff because it's saying that even though we were only trying to train this model to predict next characters, it somehow ends up learning a lot of useful structure about the input data.

EM, so one, one kind of thing that we often use. So this has not really been computer vision so far. And we need to pull this back to computer vision since this is a vision class. EM, so we've alluded many times to this image captioning model where we want to build models that can input an image and then output a caption in, in natural language. So there were a bunch of papers a couple years ago that all had relatively similar approaches, but I'm showing the figure from the paper from our lab in a totally unbiased way.

EM, uh, but uh, so the idea here is that the caption is this variably length sequence that might have, we might, this sequence might have different numbers of words for different captions. So this is a totally natural fit for a recurrent neural network language model.

So then what, what this model looks like is we have some some convolutional network which will input the, which will take as input the image. And we've seen a lot about how convolutional networks work at this point. And now that convolutional network will produce a summary vector of the image, which will then feed into the first Tim step of one of these recurrent neural network language models, which will then produce words of the caption one at a time.

So the way that this kind of works, EM, at test time after the model is trained, EM looks almost exactly the same as these character level language models that we saw a little bit ago. So we'll take our input image, feed it through our convolutional network, but now instead of taking the softmax scores from from an imageenet model, we'll instead take these, this, this 494,096 dimensional vector from the end of the model.

EM and we'll take that four ninety six dimensional model that that vector and use it to summarize the whole content of the image. Now remember when we talked about R N N language models we said that we need to see the language model with that first initial input to tell it to start generating text. So in this case we'll give it some special start token EM which is just saying hey this is the start of a sentence, please start generating some text conditioned on this this image information. So now previously we saw that in this rnn language model we had these two weight matrices that were're taking the previous the the input at the current time step and the hidden state at the previous time step and combining those to get the next hidden state, well now we also need to add in this image information. So one way to people play around with exactly different ways to incorporate this image information but one simple way is just to add a third weight matrix that is EM adding in this image information at every time step to compute the next hidden state.

So now we'll, we'll compute this distribution over all scores in our vocabulary. And here our vocabulary is something like all English words, so it could be pretty large. We'll sample from that distribution and now pass that word back as input at the next timest step. And that will will then feed that word in again, get a distribution over all words in the vocab, and again, sample to produce the next word.

So then, uh, after that thing is all done, we'll maybe generate, uh, we'll generate this, this complete sentence. So we we stop generation once we sample this special end token which kind of corresponds to the period at the end of the sentence. So then once the network samples this end token, we stop generation and we're done. And we've got our caption for this image.

And now during training, we trained this thing to generate, like we put an end token at the end of every caption during training, so that the network kind of learns during training that end tokens come at the end of sequences. So then during test time it tends to sample these end tokens once it's done generating.

EM, so we train this this model in kind of a completely supervised way. You can find data sets that have images together with natural language captions. Microsoft cocoa is probably the biggest and most widely used for this task but you can just train this train this model in a purely supervised way and then back propagate EM through to to jointly train both this recurrent neural network language model and then also pass gradients back into this final layer of the CNN and additionally update the weights of the CNN to jointly tune all parts of the model to to perform this task.

So EM, once you train these models, they actually do some pretty reasonable things. So eh, these are, these are some real results from a model from, from one of these train models. And it says things like a cat sitting in a on a suitcase on the floor, which is pretty impressive. It knows about a cat sitting on a tree branch, which is also pretty cool. It knows about two people walking on the beach with surfboards. So these models are actually pretty powerful and can produce relatively relatively complex captions to describe the image.

But that being said, these models are really not perfect. They're, they're not magical. EM, just like any any machine learning eh model. If you try to run them on data that was very different from the training data, they don't work very well. So for example, this example, it says a woman is holding a cat in her hand. EM, there's clearly no cat in the image, but she is wearing a fur coat. And maybe the texture of that coat kind of looked like a cat to the model.

Over here we see a woman standing on a beach holding a surfboard. Well she's definitely not holding a surfboard and she's doing a handstand, which is maybe the interesting part of that image and the model totally missed that. EM also over here we see this example where there's this picture of a spider web in the tree branch and it totally, it says something like a bird sitting on the tree branch so it totally missed the spider. But during training it never really saw examples of spiders. It just knows that birds sitting on tree branches during training. So it kind of makes these reasonable mistakes or here at the bottom it can't really tell the difference between this guy throwing and catching the ball. But it does know that it's a baseball player and there's balls and things involved. So I again just want to say that these models are not perfect. They work pretty well when you ask them to caption images that were similar to the training data, but they definitely have a hard time generalizing far beyond that.

So another thing you'll sometimes see is eh, this slightly more advanced model EM called attention, where now when we're generating the words of this caption, EM, we can allow the model to steer its attention to different parts of the image. EM, and I don't want, I don't want to spend too much time on this, but the general way that this works is that now our convolutional network rather than produces a single vector summarizing the entire image. Now it produces some grid of vectors that summarize the, that give maybe one vector for each spatial location in the image.

And now when we, when we, when we, when this model runs forward, in addition to sampling the vocabulary at every time step, it also produces a distribution over the locations in the image where it wants to look. And now this distribution over image locations can be seen as a kind of attention of where the model should look during training.

So now that first hidden state computes this, this distribution over image locations which which then goes back to the set of of vectors to give a single summary ve that maybe focuses the attention on one part of that image. And now that summary vector gets Fed as an, as an additional input at the next time step of the neural network. And now again it will produce two outputs. One is our distribution over vocabulary words and the other is a distribution over image locations.

And this thing, this whole process will continue. And it will sort of do these two different things at every time step.

EM, and after you train the model, then you can see that it kind of eh will will shift its attention around the image for every word that it generates in the caption. So here you can see that it, it produced the caption a bird is flying over. I can't see that far, but you can see that it its attention is shifting around different parts of the image for each word in the caption that it generates.

EM, and there's this notion of hard attention versus soft attention, which I don't really want to get into but too much. But with this idea of soft attention, we're kind of taking a weighted combination of all image, location of all features from all image location, whereas in the hard attention case, we're forcing the model to select exactly one location to look at in the image at each time step.

EM, so the heart attention case where we're selecting exactly one image location is a little bit tricky because that that is not really a differentiable function. So you need to do something slightly fancier than vanilla back propagation in order to train the model in that scenario. And I think we'll talk about that a little bit later when we end the lecture on reinforcement learning.

So now when when you look at after you train one of these attention models and then run it on to generate captions, you can see that it tends to focus its attention on maybe the salient or semantically meaningful part of the image when generating captions. So you can see that the caption was a woman is throwing a frisbee in a park and you can see that this attention mask, when it generated the word, when it when the model generated the word frisbee at the same time it was focusing its attention on this image region that actually contains the frisbee.

So this is actually really cool. We did not tell the model where it should be looking at every time stop. It sort of figured all that out for itself during the training process because somehow it figured out that looking at that image region was the right thing to do for this image. And because everything in this model is differentiable because we can back propagate through all these soft attention steps, all of this soft attention stuff just comes out through through the training process.

So that's really, really cool.By the way, this idea of recurrent neural networks and attention actually gets used in other tasks beyond image captioning. So one recent example is this idea of visual question answering. So here our model is going to take two things as input. It's going to take an image and it will also take a natural language question that's asking some question about the image. So here we might see this image on the left and we might ask the question, what endangered animal is featured on the truck? And now the model needs to select from one of these four natural language answers about which of these answers correctly answers that question in the context of the image.

So you can imagine kind of stitching this model together using cn n and r and n in kind of a natural way. So now we're in this uh this this many to one scenario. EM, where now our model needs to take as input this natural language sequence. So we can imagine running a recurrent neural network over each element of that input question. To now summarize the input question in a single vector and then we can have a CNN to again summarize the image and now combine both the vector from the CNN and the vector from, from the question encoding rnn to then predict a distribution over answers. EM, we also sometimes you'll also sometimes see this idea of soft spatial attention be incorporated into things like visual question answering.

EM, so you can see that here, this this model is also having the spatial attention over the image when it's trying to determine answers to the questions. So just yeah, question.

So the question is, how are the different inputs combined? Do you mean like the encoded question vector and the encoded image vector?

So the, the question is how are, how are the encoded image and the encoded question vector combined? Kind of the simplest thing to do is just to concatenate them and then stick them into fully connected layers. That's probably the most common, and that's probably the first thing to try. Sometimes people do slightly fancier things or they might try to have multiplicative interactions between those two vectors to allow a more powerful function. But generally, concatenation is kind of a good first thing to try.

OK, so now we've talked about a bunch of scenarios where r and n are used for different kinds of problems, and I think it's super cool because it allows you to start tackling really complicated problems, combining combining images and computer vision with natural language processing. And you can see that we can kind of stitch together these models like Lego blocks and attack really complicated things like image captioning or visual question answering just by stitching together these relatively simple types of neural network modules.

Um.But I'd also like to mention that uh, so far we've talked about this idea of a single recurrent neural network layer where we have sort of one hidden state, EM, and another thing that you'll see pretty commonly is this idea of a multi layer recurrent neural network. So here we this is a three layer recurrent neural network. So now our input goes in, goes into goes in and produces a sequence of hidden states from the first recurrent neural network layer. And now EM after we run kind of one recurrent neural network layer. Then we have this whole sequence of hidden states EM and now we can use this sequence of hidden states as an input sequence to another recurrent neural network layer and then you can just which will then produce another sequence of hidden states from the second rnn layer and then you can just imagine stacking these things on top of each other because we know that we've seen in other contexts that deeper models tend to perform better for various problems and the same kind of holds in r and n as well. So EM for many problems you'll see that maybe a two or three layer recurrent neural network model is pretty commonly used, EM you typically.

Don't see super deep models in r and n, so generally like two, three, four, layer r and n is maybe as deep as you'll typically go.

So then, eh, I think it's, it's also really interesting and important to think about EM. Now we've seen kind of what kinds of what kinds of problems these R N N can be used for, but then you need to think a little bit more carefully about exactly what happens to these models when we try to train them. So here I've drawn this little vanilla r and n cell that we've talked about so far. So here we're taking our current input X T, EM and our previous hidden state H T minus one and then we stack, we stack, those are two vectors EM. So we can just stack them together and then perform this matrix multiplication with our weight matrix EM to give our and then squash that output through a ten h and that will give us our next hidden state. And that's kind of the basic functional form of this vanilla recurrent neural network.

But then we need to think about what happens to what happens in this architecture during the backward pass, when we try to compute gradients.

So then if we think about trying to compute, so then during the backwards pass we'll receive EM, the derivative of our ht, we'll receive derivative of loss with respect to ht. And during the backward pass through this cell, we'll need to compute derivative of loss with respect to ht minus one, so then when we compute this backward pass, we see that the gradient flows backwards through this red path. So first that gradient will flow backwards through this ten h gate and then it will flow backwards through this matrix multiplication gate.

And then, as we've seen in the homework and when implementing these, these matrix multiplication layers, when you back propagate through this matrix multiplication gate, you end up multiplying by the transpose of that weight matrix. So that means that every time we back propagate through this through one of these vanilla rn n cells, we end up multiplying by, by some part of the weight matrix.

So now if you imagine that we are sticking many of these recurrent neural network cells in sequence because again this is an r and n we want to model sequences. Now if you imagine what happens to the gradient flow through a sequence of these layers, then something kind of fishy starts to happen because now when we want to compute the gradient of the loss with respect to h zero, we need to back propagate through every one of these rn n cell. And every time you back propagate through one cell you'll pick up one of these w transpose factors. So that means that the, the final expression for the gradient on h zero will involve many, many factors of this weight matrix, which could be kind of bad.

EM, so maybe don't think about the, the, the matrix case. But imagine a scalar case. If we end up, if we have some scalar and we multiply by that same number over and over and over again, maybe not for four examples, but for something like a hundred or several hundred time time stops, then multiplying by the same number over and over again is really bad. EM, in the scalar case, it's either going to explode in the case that that number is greater than one or it's going to vanish towards zero in the case that that number is less than one.

In absolute value, and the only way in which this will not happen is if that number is exactly one, which is actually very rare to happen in practice. So that leads us to this so that that same intuition extends to the matrix case, but now, rather than the absolute value of a scalar number, you instead need to look at the largest, the largest singular value of this weight matrix.

So now if that largest singular value is greater than one, then during this backward pass, when we multiply by the weight matrix over and over that gradient on hw on h zero, sorry, will become very, very large when that matrix is too large. And that's, that's something we call the explodeing gradient problem, where now this gradient will will explode exponentially in depth, eh, with the number of time steps that we back propagate through. And if the largest singular value is less than one, then we get the opposite problem, where now our gradients will shrink and shrink and shrink exponentially as we back propagate and pick up more and more factors of this weight matrix.

So EM, that's called the vanishing gradient problem. So there's a bit of a hack that people sometimes do to fix the exploding gradient problem called gradient clipping, which is just this simple heuristic saying that after we compute our gradient, if that gradient, if its l2 norm is above some threshold, then just clamp it down and divide and just clamp it down so it has this, this maximum threshold. So this is kind of a nasty hack, but it actually gets used in practice quite a lot when training recurrent neural networks and it's a relatively useful tool for, for attacking this exploding gradient problem.

But now for the, for the vanishing gradient problem, what we typically do is we might need to move to a more complicated rnn architecture.

So, that that motivates this idea of an ls T M EM an L S T M is a slightly which stands for long short term memory is this slightly fancier recurrence relation for uh for these recurrent neural networks that is really designed to help alleviate this problem of vanishing and exploding and exploding gradients. So that rather than kind of hacking on top of it, EM, we just kind of designed the architecture to have better gradient flow properties, kind of in analogy to those fancyer see it on architectures that we saw at the top of the lecture.

So eh, another thing to point out is that this lsstm cell actually comes from 1997 EM. So this idea of an lstm has been around for quite a while, EM and, and these folks who were working on these ideas way back in the nineties were definitely ahead of the curve because these models are kind of used everywhere now, twenty years later.

EM and and ls tms have kind of this funny functional form and one thing about, so remember when we had this vanilla recurrent neural network it had this hidden state and we used this recurrence relation to update the hidden state at every time step. Well, now in an ls tm we actually have two, we maintain two hidden states at every time. Step one is this ht which is called the hidden state EM, which is kind of in analogy to the hidden state that we had in the vanilla R N N.

But an ls tm also maintains this second vector CT called the cell state and the cell state is this vector which is kind of internal kept inside the ls tm and it it does not really get exposed to the outside world. And we'll see. You can kind of see that through this update equation where EM you can see that when we first we compute these we use, we take our two inputs, we use them to compute these these four eh gates called if F O and g, we use those gates to update our cell state CT and then we expose part of our cell state, eh, as the hidden state at the time at the next time stop.

So when we in, this is kind of a funny, functional form and I want to walk through for a couple of slides, like exactly why do we use this architecture and why does it make sense, especially in the context of vanishing or exploding gradients.

So then eh this first, this first thing that we do in an ls tm is that we're given this previous hidden state ht and we're given our current input vector X T EM, just like the vanilla rn n in the vanilla rn. N remember we took those two, those two input vectors, we concatenated them. Then we did a matrix multiply to directly compute kind of the next hidden state in the rnn. Now the lsstm does something a little bit different. So we're going to take our previous hidden state and our current input, stack them and now multiply by a very by a big weight matrix w to compute four different gates EM which all have the same size as the hidden state EM. Sometimes you'll see this written in different ways. Some authors will write different weight mat, different weight matrix for some, for each gate. Some authors will combine them all into one big weight matrix but it's all really the same thing. The idea is that we take our hidden state, our current input and then we use those to compute these four gates.

EM these four gates are the I you'll often see this written as I F O G I fog which makes it pretty easy to remember what they are. So then I is the I is the input gate. It says how much do we want to input into our cell EM f is the forget gate. How much do we want to forget the cell memory at the previous from the previous time step o is the output gate which is how much do we want to reveal our cell to the outside world and g doesn't really have a nice gate, a nice name so I usually call it the gate gate and, and and g it tells us how much do we want to write into our input cell.

And then you notice that each of these four gates have a different are using a different nonlinearity so the.

So the input for get and output gate are all using sigmoids, which means that their values will be between zero and one, whereas the gate gate uses A T H, which means its output will be will be between minus one and one.

So these are kind of weird, but it makes a little bit more sense if you imagine it, if you imagine them all as binary values, right? Like what happens at the extremes of these two values? So kind of what happens if you look after, we compute these gates. If you look at this next equation you can see that our cell state is being multiplied element wise by the forget gate. Sorry, our cell state from the previous time stop is being multiplied element wise by this forget gate. And now if this forget gate, if you you can think of it as being a vector of zeros and ones. So that's telling us for each element in the cell state do we want to forget that element of the cell if in the case that the forget gate was zero or do we want to remember that element of the cell in the case that the forget gate was one?

Now once weve once we've used the forget gate to gate off the the part of the cell state, then we have the second term which is the the element wise product of I and g, so now I EM is this vector of zeros and ones because it's coming through a sigmoid telling us for each element of the, of the cell state, do we want to write to that element of the cell state in the case that I is one or do we not want to write to that element of the cell state at this time step in the case that I, that I I is zero.

And now the gate gate, because it's coming through a can h will be either one or minus one, so that is the value that we want to the the candidate value that we might consider writing to the cell, to each element of the cell state at this time step. So then if you look at the cell state equation, you can see that at every time step, the cell state has these of these different independent scalar values and they're all being incremented or decremented by one.

So there's kind of like the cell inside the cell state. We can either remember or forget our previous state, and then we can either increment or decrement each element of that cell state by up to one at each time step. So you can kind of think of this cell, these elements of the cell state as being like little scalar integer counters that can be incremented and decremented at each time step.

And now EM, after we've computed our our cell state, then we use our our now updated cell state to compute a hidden state which we will which we will reveal to the outside world. So, so because this cell state has this interpretation of being counters and and sort of counting up by one or minus one at each time step. We want to squash that counter value into kind of a nice zero to one range using a tan h and now we multiply element wise by this output gate and the output gate is again coming through a sigmoid. So you can think of it as being mostly zeros and ones and the output gate tells us for each element of our cell state do we want to reveal or not reveal that element of our cell state when we're computing the the external hidden state for this time step.

EM, and then I think there's, there's kind of a tradition in people trying to explain l sstms that everyone needs to come up with their own potentially confusing lsst m diagram EM. So here's my attempt. So here we can. We can see what's going on inside this ls tm cell is that we're taking as input on the left the previous cell state and the previous hidden state as well as our current input X T, now we're going to take our current, our previous, previous hidden state as well as our current input, stack them and then multiply with this weight matrix w to produce our four gates. And here I've left out the nonlinearities because we saw those on the previous slide.

And now the forget gate multiplies element wise with the cell state. The input and gate gate are multiplied element wise and added to the cell state and that gives us our next cell. The next cell gets squashed through a ten h and multiplied element wise with this output gate to produce our next hidden state.

Question.So they're coming through this right, they're coming from different parts of this weight matrix. So if our, if our hidden, if our, if our X and our h all have this, this dimension h, then after we stack them there'll be a vector of size two h and now our weight matrix will be this matrix of size four h times two h so you can think of that as sort of having like four chunks of this weight matrix, one chunks of this weight and each of these four chunks of the weight matrix is going to compute a different, different one of these gates.

So EM, you'll often see this written for clarity, kind of combining all four of those different weight matrices into a single large matrix w just for notational convenience EM, but they're all, they're all computed using different parts of the weight matrix.

But, but you're correct in that they're all computed of using the same functional form of just stacking the two things and taking a matrix multiplication.

EM, so now if we now that we have this picture we can think about EM what happens to an L S tm cell during the backwards pass. We saw in the context of a vanilla recurrent neural network that some bad things happened during the backwards pass where we were continually multiplying by that weight matrix w but now the situation looks much quite a bit different in the lsstm. So if you imagine this this path of this path backwards of computing the gradients of the cell state we get quite a nice picture. So now when we have our upstream gradient from the cell coming in, then eh once once we back propagate backwards through this addition operation, remember that this addition just copies that that upstream gradient into the two branches. So our upstream gradient gets copied directly and passed directly to back to back propagating through this element wise multiply. So then our upstream gradient ends up getting multiplied element wise by the forget gate.

So EM as we as we back propagate backwards through this cell state. The only thing that happens to our upstream cell state cell state gradient is that it ends up getting multiplied element wise by the forget gate. So this is much, this is really a lot nicer than the vanilla r and n for two reasons. One is that this forget gate is now an element wise multiplication rather than a full matrix multiplication. So element wise multiplication is going to is going to going to be a little bit nicer than full matrix multiplication. Second is that that element wise multiplication will will potentially be multiplying by a different forget gate at every time step.

So remember in the vanilla rn n we were continually multiplying by that same weight matrix over and over again, which LED very, very explicitly to these exploding or vanished ingredients. But now in the ls tm case, EM this, this forget gate can vary from each time step. So now it's much, it's much easier for the model to to avoid these problems of exploding and vanished gradients. EM finally, because this forget gate is coming out from a sigmoid, this element wise multiply is guaranteed to be between zero and one, which again leads to sort of nicer numerical properties if you imagine multiplying by these things over and over again.

Another thing to notice is that EM in the context of the vanilla recurrent neural network, we saw that during the backward pass our gradients were flowing through also a tan h at every time step, but now in an lsstm our outputs are like because in an lstm the hidden state is used to compute those outputs yt. So now EM each hidden state. If you imagine back propagating from the final hidden state back to the first cell state, then through that backward path we only back propagate through a single ten H A single ten h non linearity rather than through a separate ten h at every time step.

So kind of when you put all these things together, EM, you can see this backwards pass back propagating through this through this cell state as kind of a gradient superhighway that lets gradients pass relatively unimpeded from the loss at the very end of the model all the way back to the initial cell state at the beginning of the model. So yeah, was there a question.

Yeah, what about the gradient with respect to w? So because that's ultimately the thing that we care about. So the gradient with respect to w will come through. Eh, right? So we'll at every time step we'll take our current cell state as well as our current hidden state and that will give us an element that will give us our local gradient on w for that time step. So because our cell state and and then just in the vanilla r and n case, we'll end up adding those per time. Step w gradients to compute our final gradient on w.

But now, because uh, so if you imagine the situation where we have a very long sequence and we're only getting gradient at the very end of the sequence now as you back propagate through, we'll get a local gradient on w for each time step EM, and that local gradient on w will be coming through these gradients on c and h, so because we're maintaining the gradients on c much more nicely in the ls tm case, those local gradients on w at each time step will also be carried forward and backward through time much more cleanly.

Another question.So the question is due to the nonlinearities could this still be susceptible to vanish ingredients? EM, and that could be the case EM actually so one problem you might imagine is that maybe if these forget gates are always less than zero are always less than one, then you might get vanish ingredients as you continually go through these forget gates. Well, one sort of trick that people do in practice is that they will sometimes initialize the biases of the forget gate to be somewhat positive so that at the beginning of training those forget gates are always very close to one EM. So that at least at the beginning of training then then we have not so relatively relatively clean gradient flow through these forget gates since they're all initialized to be near one.

EM, and then throughout the course of training, then the model can learn those biases and kind of learn to forget where it needs to. So you're right that there still could be some potential for banishing ingredients here, but it's much less extreme than the vanilla rn n case, both because those f can vary at each time step, and also because we're doing this element wise multiplication rather than a full matrix multiplication.

So you can see that this, this ls T M actually looks quite similar to resonances. So in, in this residual network, we had this path of identity connections going backward through the network and that gave sort of a gradient superhighway for gradients to flow backward in resonets. And now it's kind of the same intuition in an ltm where these additive, these additive and element wise multiplicative interactions of the cell state kind of give a similar gradient superhighway for gradients to flow backwards through the cell state in an ltm.

EM, and by the way, there's, there's this other kind of nice paper called highway networks which is kind of in between this idea of, of this ls tm cell and this and in these in these residual networks. So these highway networks actually came before residual networks. EM, and they were, they had this idea where at every layer of the highway networks we're going to compute sort of a candidate activation as well as a gating function that tells us that interpolates between our, our, previous, our input at that layer and that candidate activation that came through our convolutions or whatnot. So there's actually a lot of architectural similarities between these things. And people take a lot of inspiration from training very deep cnns and very deep rnns and there's a lot of crossover here.

EM very briefly EM you'll you'll see a lot of other types of variance of recurrent neural network architectures out there in the wild. Probably the most common apart from the ls T M is this g ru called the gated recurrent unit EM and you can see those update equations here and it kind of has this similar flavor of the ls T M EM where it uses these these multiplicative element wise gates together with these additive interactions to avoid the Spanish ingredient problem. EM there's also this this cool paper called ls T M A search space odyssey where a very inventive title EM where they tried they tried to play around with the ls tm equations and swap out the nonlinearities at one point like do we really need that ten h for exposing the output gate and they tried to answer a lot of these different questions about like for each of those nonlinearities each of those pieces of the ls tm update equations. What happens if we change the model and tweak those lsdm equations a little bit EM and kind of the conclusion is that they all work about the same EM. Some of them work a little bit better than others for other for one problem or another but generally none, none of the things that none of the tweaks of lsstm that they.

Tried were significantly better than the original lsstm for all problems, so that that gives you a little bit more faith that the lsstm update equations seem kind of magical. But they're useful anyway. You should probably consider them for your problem. There's also this cool paper from Google a couple years ago where they tried to use, they did kind of an evolutionary search and did a search over many over a very large number of random, uh, rnn architectures to kind of like randomly permute these update equations and try putting the editions and the multiplications and the gates and nonlinearities in different kinds of combinations. They blasted this out over the huge Google cluster and just tried a whole bunch of these different weight updates in various in various flavors. And again it was the same story that they didn't really find anything that was significantly better than these existing G U or l tm cells, although there were some variations that worked maybe slightly better or worse for certain problems.

EM, but kind of the takeaway is that probably using an lstm or a Gu is maybe not such. There's not so much magic in those equations, but this idea of managing gradient flow properly through these additive connections and these multiplicative gates is super useful.

EM. So yeah, the summary is that R N N are super cool. They can allow you to attack tons of new types of problems. They sometimes are susceptible to vanishing or exploding gradients, EM. But we can address that with weight clipping and with fancier architectures. And there's a lot of cool overlap between CNN architectures and rnn architectures.

So next time you'll be taking the midterm. But after that we'll have sorry, question.It terms up to this lecture, so anything up to this point, a spare game.

And see you guys. Good luck on the midterm on tuesday.