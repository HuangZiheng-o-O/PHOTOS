Stanford university hello. Okay, it's after twelve so I want to get started. So today lecture eight we're going to talk about deep learning software. EM this is a super exciting topic because it changes a lot every year. EM, that also means it's a lot of work to give this lecture because it changes a lot every year EM but as usual a couple of administrative notes before we dive into the material. So as a reminder the project proposals for your course projects were due on tuesday so hopefully you all turned that in and hopefully you all have a somewhat good idea of what kind of projects you want to work on for the class EM. So we're in the process of assigning tas to projects based on what the project area is and the expertise of the tas. So we'll have some more information about that in the next couple days I think.

EM we're also in the process of grading assignment one so stay tuned and we'll get those grades back to you as soon as we can. EM another reminder is that assignment two has been out for a while. That's going to be due next week, a week from today, thursday EM and again when working on assignment two remember to stop your Google cloud instances when you're not working to try to preserve your credits. EM and and another another bit of confusion I just wanted to reemphasize is that EM? For assignment two you really only need to use gpu instances for the last notebook for all of the for of the all the first several notebooks. It's just eh in python and numpy so you don't need any gpus for those questions. So eh again conserve your credits. Only use G P U when you need them.

EM, and the final reminder is that the the midterm is coming up. It's kind of hard to believe we're there already, but the midterm will be in class on tuesday. Five, nine, EM. So the midterm will be more theoretical. It'll be sort of pen and paper, working through different kinds of more slightly more theoretical questions to check your understanding of the material we've covered so far. And I think we'll probably post at least a short sort of sample of the types of questions to expect.

Question.Oh.Yeah, question is whether it's open book. So we're going to say close note, close book, so just.

Yeah, yeah. So that's what we've done in the past is just closed note, closed book, relatively just like want to check that you understand the intuition behind most of the stuff we've presented.

Um.So a quick recap as a reminder of what we were talking about last time, eh? Last time we talked about fancier optimization algorithms for deep learning models, eh, including s gd, momentum, nestor of rms, prop and atom. And we saw that these relatively small tweaks on top of on top of vanilla s gd EM are relatively easy to implement but can make your make your networks converge a bit faster. We also talked about regularization, especially dropout. So remember dropout. You're kind of randomly setting parts of the network to zero during the forward pass and then you kind of marginalize out over that, that noise in the back at test time.

And we saw that this was kind of a general pattern across many different types of regularization and deep learning, where you might add some kind of noise during training, but then marginalize out that noise at test time so it's not stochastic at test time.

We also talked about transfer learning, where you can maybe download big networks that were pre trained on some dataset and then fine tune them for your own problem. And this is one way that you can attack a lot of different problems in deep learning, even if you don't have a huge data set of your own.

So today we're going to shift gears a little bit and talk about some of the nuts and bolts about writing software and, and how the hardware works, and a little bit diving into a lot of details about what the software looks like that you actually use to train these things in practice. So we'll talk a little bit about cpus and gpus and then we'll talk about several of the major deep learning frameworks that are out there in use these days.

So first, EM, we've, we've talked, we've sort of mentioned this offhand a bunch of different times that there's like computers have cpus computers, have gpus deep learning uses gpus. But we weren't really too explicit up to this point about what exactly these things are and why one might be better than another for different tasks.

So who built a computer before? Just kind of show a hands.So maybe, maybe about a third of you, half of you somewhere around that ballpark. So this is, this is a shot of my computer at home that I built. And you can see that there's a lot of stuff going on inside the computer. Maybe, hopefully, you know what most of these parts are.

EM and the cpu is the central processing unit. That's this little chip hidden under this cooling fan right here near the near the top of the case. EM, and it, the cpu is actually relatively small piece, like it's a relatively small thing inside the case. It's, it's not taking up a lot of space.

EM and the gpus are these two big monster things that are taking up a gigantic amount of space in the in the space. In the case they have their own cooling. They're taking a lot of power, they're quite large EM. So just in terms of like how much power they're using in terms of how big they are, the G P U are kind of physically imposing and taking up a lot of space in the case. So the question is like, what are these things and why are they, why are they so important for deep learning?

Well, the G P U is is called a graphics card or graphics processing unit and these were really developed EM in originally for rendering computer graphics and especially around games and that sort of thing. So another show of hands who's, who's got who's plays video games at home sometimes from time to time on their computer.

Yeah, so again, like maybe about half good fraction. So for those of you who've played video games before and who built your own computers, you probably have your own opinions on this debate. So this is one of those, like big debates in, in computer science is like, you know, there's like Intel versus AMD and and video versus AMD for graphics cards, maybe up there with vim versus emacs for text editor. And pretty much any gamer has their own opinions on which of these two sides they prefer for their own cards. And in deep learning we kind of have mostly picked one side of of this, of this fight and that's Nvidia.

So if you guys have AMD cards, you might be in a little bit more trouble if you want to use those for deep learning. And really, Nvidia has been pushing a lot for deep learning in the last several years. It's been kind of a large focus of some of their strategy and they put a lot of effort into engineering sort of good solutions to make their hardware better suited for deep learning. So there's so most people in deep learning. When we talk about gpuss EM, we're pretty much exclusively talking about Nvidia gpuss. EM, maybe in the future this will change a little bit and there might be new players coming up, but at least for now, Nvidia is pretty dominant.

So to give you an idea of like what is the difference between a cpu and a gpu EM, I've kind of made a little spreadsheet here. On the top we have EM two of the kind of top end Intel consumer cpus. And on the bottom we have two of n videos, sort of current top end consumer gpus. And there's a couple general trends to look to notice here. EM, both gpus and cpus are kind of a general purpose computing machine where they can execute programs and do sort of arbitrary instructions, but they're qualitatively pretty different.

EM, so cpus tend to have just a few cores, EM for consumer, for consumer desktop cpus these days they might have something like four, six or maybe up to ten cores. EM with with hyper threading technology that means they can run. The hardware can physically run like maybe eight or up to twenty threads concurrently. So the cp u can maybe do twenty things in parallel at once. EM, so this is not a gigantic number but those those threads for a cpu are pretty powerful. They can actually do a lot of things. They're very fast. Every cpu instruction can actually do quite a lot of stuff and they can all work pretty independently.

EM for g puss the it's a little bit different. So for g pus we see that these sort of common top end consumer g puss have thousands of cores. So the the Nvidia titan xp which is the current top of the line consumer gpu has 3840 cores. So that's, that's a crazy number. That's way more than the ten cores that you'll get for a similarly priced cpu.

EM, the downside of a gpu is that each of those cores, one, it runs at a much lower clock speed, EM and two, they really can't do quite as much. You can't really compare cpu cores and gpu cores, apples to apples. EM, the gpu cores can't really operate very independently. They all kind of need to work together and sort of paralyze one task across many cores rather than each core totally doing its own thing. So you can't really compare these these numbers directly, but it should give you the sense that EM, due to the large number of cores g pus can sort of are really good for parallel things where you need to do a lot of things all at the same time, but those things are all pretty much the same flavor.

EM another thing to point out between cpus and gpus is is this idea of memory, right? So cpuss have some um, some cache on the, on the cpu but that's relatively small and the majority of the memory for your cpu is is pulling from your system memory the Ram which will maybe be like eight, twelve, sixteen, thirty two gigabytes of Ram on a typical consumer desktop these days, EM, whereas gpus actually have their own Ram built into the chip. EM, they there's a pretty large bottleneck communicating between the Ram in your system and the and the gpu so that gpus typically have their own relatively large block of memory within the card itself.

EM, and for the the titan xp, which again is the maybe the current top of the line consumer card, this thing has about has twelve gigabytes of memory local to the gpu gp. Uus also have their own caching system where there's sort of multiple hieries of caching between the twelve gigabytes of gpu memory and the actual gpu cores. And that's somewhat similar to the caching hierarchy that you might see in a cpu.

Um, so cpus are kind of good for general purpose processing. They can do a lot of different things, and gpus are maybe more specialized for these highly paralyzeable algorithms.

So the prototypical algorithm of something that works really, really well and is perfectly suited to a gpu is matrix multiplication. So remember in matrix multiplication on the left we've got like a matrix composed of a bunch of rows. We multiply that on the right by another matrix composed of a bunch of columns, and then this produces another, a final matrix where each element in the output matrix is a dot product between one of the rows and one of the columns of the two input matrices. And these dot products are all independent, like you could imagine sort of for this output matrix. You could split it up completely and have each of those different elements of the output matrix all being computed in parallel and they all sort of are running the same computation which is taking a dot product of these two vectors. But exactly where they're reading that data from is from different places in the two input matrices. So you can imagine that for a gpu you can just like blast this out and have all of this elements of the output matrix all computed in parallel and that could make this this thing actually compute super, super fast on gpu.

So that's kind of the prototypical type of problem that like where a gpu is really well suited, where a cpu might have to go in and step through sequentially and compute each of these elements one by one, um, that that picture is a little bit, a little bit of a caricature because cpus these days have multiple cores. They can do vectorized instructions as well. Um, but still for these like massively parallel problems, G P U tend to have much better throughput, especially when these matrices get really, really big.

Um, and by the way, convolution is kind of the same kind of story where, you know in convolution we have this input tensor, we have this weight tensor, and then every point in the output tensor after convolution is again some inner product between some part of the weights and some part of the input. And you can imagine that a gpu could really paralyze this computation, split it all up across the many cores and compute it very quickly. So that's, that's kind of the general flavor of the types of problems where g pus give you a huge speed advantage over over cpus.

EM so you can actually write programs that run directly on gpus. So Nvidia has this coa abstraction which EM lets you write code that kind of looks like c but executes directly on the gpus EM. But writing CA code is really, really tricky. EM, it's, it's actually really tough to write coa code that's performanceant and actually squeezes all the juice out of the of these gpus. You have to be very careful managing the memory hierarchy and making sure you don't have cache misses and branch Miss predictions and all that sort of stuff. So it's actually really, really hard to write performance Co to code on your own. So as a result Nvidia has released a lot of libraries that implement common computational primitives that are very, very, very highly optimized for gpus. So for example, Nvidia has a cou blast library that implements different kinds of matrix multiplications and different matrix operations that are super optimized run really well on gpu get very close to sort of theoretical p hardware utilization. EM similarly they have a code dnn library which implements things like convolution forward and backward passes.

Is batch normalization, recurrent networks, all these kinds of computational primitives that we need in deep learning? Andvidia has gone in there and released their own binaries that compute these primitives very efficiently on Nvidia hardware. So in, but so in practice, you tend not to end up writing your own code to code for deep learning. You typically are just mostly calling into existing code that other people have written, much of which is this stuff which has been heavily optimized by Nvidia already.

EM there's another. There's another sort of language called open cl which is a bit more general runs on more than just Nvidia gpus. EM can run on AMD hardware can run on cpus EM, but open cl doesn't. Nobody's really spent a really large amount of effort and energy trying to get optimized deep learning primitives for open cl. So that tends to be a lot less performance than than the the super optimized versions in ka. So EM maybe in the future we might see a bit of a more open standard and we might see this across many different more types of platforms, but at least for now this Nvidia is kind of the main game in talent for deep learning.

So you can check there's a lot, a lot of different resources for learning about how you can do gpu programming yourself. It's kind of fun. It's sort of a different paradigm of writing code because it's this massively parallel architecture, EM, but that's a bit beyond the scope of this course. And again, you, you don't really need to write your own code to code much in practice for deep learning. And in fact, I've never written my own code to code for any research project. So, but it is kind of useful to know how it works and what are the basic ideas, even if you're not writing it yourself.

So if you want to look at EM kind of c pu gpu performance and practice EM, I did some benchmarks last summer comparing eh relatively like a decent Intel cpu against a bunch of different gpus that were sort of near top of the line at that time, EM. And these were my own benchmarks that you can find more details on github, but my findings were that like for things like vgg sixty and nineteen resonets various resonets then you typically see like something like a 65 to 75 times speed up when going when running the exact same computation a jeep on a top of the line gpu in this case a Pascal tightx versus a top of the line. Well, not quite top of the line cpu, which in this case was EM.

Uh, an Intel e five processor, EM. Although I'd like to make one sort of caveat here is that you always need to be super careful whenever you're reading any kind of benchmarks about deep learning because it's super easy to be unfair between different things and you kind of need to know a lot of the details about what exactly is being benchmarked in order to know whether or not the comparison is fair. So in this case I'll come come right out and tell you that probably this comparison is a little bit unfair to cpu EM because EM I I didn't spend a lot of effort trying to squeeze the maximal performance out of cpus. I probably could have tuned the blast libraries better for the cpu performance and I probably could have gotten these numbers a bit better. EM, this was sort of out of the box performance between like just installing torch, running it on a cpu, just installing torch, running it on a gpu. So this is kind of out of the box performance but it's not really like peak possible theoretical throughput on the cpu.

But that being said, I think there are still pretty substantial speed UPS to be had here.Another another kind of interesting outcome from this benchmarking was EM comparing these optimized k dnn libraries from Nvidia for convolution and whatnot versus sort of more naive coa that had been handwritten out in the open source community EM. And you can see that if you compare the same networks on the same hardware with the same deep learning framework and the only difference is swapping out these k dnn versus sort of handwritten, less optimized coa, you can see something like nearly a three X speed up across the board when you switch from this relatively simple coda to these super optimized Q D and n implementations. So in general, whenever, whenever you're writing code on gpu you should probably almost always like just make sure you're using Co and n because you're leaving probably a three X performance boost on the table if you're not calling into Co d and n for your stuff.

So another problem that comes up in practice when you're training these things is that EM, you know, your model is maybe sitting on the gpu EM. The weights of the model are in that twelve gigabytes of local storage on the gpu, but your big dataaset is sitting over on the right on a hard drive or an ssd or something like that. So um, if you're not careful you can actually bottleneck your training by just trying to read the data off the off the off the disks. Because the gpu is super fast, it can compute forward and backward quite fast. But if you like you're reading sequentially off off a spinning disk, you can actually bottleneck your training quite and that can be really bad and slow you down.

EM, so some solutions here are that like you know if your dataaset's really small, sometimes you might just read the whole dataet into Ram or even if your dataaset isn't so small but you have a giant server with a ton of Ram, you might do that anyway EM. You can also make sure using an ssd instead of a hard drive that can help. That can help a lot with read throughput. EM another common strategy is to use multiple threads on the cpu that are prefetching data off Ram are off disk, buffering it in memory in Ram so that then you can continue feeding that buffer data down to the gpu with with good performance. EM, this is a little bit painful to set up but again like these gpus are so fast that if you're not really careful with trying to feed them data as quickly as possible, just reading a data can sometimes bottleneck the whole training process.

So that's something to be aware of.So that's, that's kind of the brief introduction to like sort of gpu cpu hardware in practice when it comes to deep learning. And then I wanted to switch gears a little bit and talk about the software side of things, the various deep learning frameworks that people are using in practice. But I guess before I move on, is there any sort of questions about cpu gpu?

Yeah, question.Yeah, so the question is what can you sort of what can you do mechanically when you're coding to avoid these problems? EM probably the biggest thing you can do in software is set up sort of pref fetching on the cpu like you couldn't like sort of a naive thing would be. You have this sequential process where you first read data off disk, wait for data, wait for the mini batch to be read, then feed the mini batch to the gpu then go forward and background the g ppu then read another mini batch and sort of do this all in in sequence in sequence. And if you actually have multiple, like instead you might have cpu threads running in the background that are fetching data off the disk such that while the while you can sort of interleave all of these things like the gpu is computing, the cpu background threads are feeding data op disk and your main thread is kind of waiting for these things to just doing a bit of synchronization between these things so they're all happening in parallel.

And, and, and thankfully, if you're using some of these deep learning frameworks that we're about to talk about, then some of this work has already been done for you because it's a little bit painful.

So eh the the landscape of deep learning frameworks is super fast moving EM. So last year when I gave this lecture I talked mostly about cafe torch theo and tensorflow EM and when I last gave this talk about again more than a year ago tensorflow was relatively new. EM, it had not seen like super widespread adoption yet at that time EM. But now I think in the last year eh tensorflow has gotten much more popular. It's it's probably the main framework of choice for many people. So eh that's that's a big change EM. We've also seen a ton of new frameworks sort of popping up like mushrooms in the last year EM. So in particular cafe two and pytorch are new frameworks from Facebook that I think are pretty interesting. EM there's also a ton of other frameworks paddle like Baidu has paddle, Microsoft has cn k EM, Amazon is mostly using mx mxnet and there's a ton of other frameworks as well that I I'm less familiar with and really don't have time to get into.

EM but one interesting thing to point out from this picture is that eh kind of the the first generation of deep learning frameworks that really saw wide adoption were built in academia. So cafe was from Berkeley, torch was developed originally at nyu and also in collaboration with Facebook and thena was mostly built at university of Montreal. But um these kind of next generation deep learning frameworks all originated in industry. So cafe two is from Facebook, py torches from Facebook, tensorflow is from Google. So that's kind of an interesting shift that we've seen in the landscape over the last couple of years is that these these ideas have really moved a lot from academia into industry and now industry is kind of giving us these big, powerful, nice frameworks to work with.

EM, so the, so today I wanted to mostly talk about pytorch and tensorflow because I I personally think that those are probably the ones you should be focusing on for a lot of research type problems. These days. I'll also talk a bit about cafe and cafe two, but probably a little bit less emphasis on those.

EM, and before we move any farther, I thought I should make my own biases a little bit more explicit. EM, so I've mostly, I've worked with torch mostly for the last several years and, and I, I've used it quite a lot. I like it a lot. And then I in in the last year I've mostly switched to pytorch as my main research framework, so I have a little bit less experience with some of these others, especially tensorflow. But I'll still try to do my best to give you a fair picture and a decent overview of these things.

So, eh, so remember that in the last several lectures we've hammered this idea of computational graphs in sort of over and over that whenever we're doing deep learning, you want to think about building some computational graph that computes whatever function that you want to compute. So in the case of a linear classifier, you'll combine your data X and your weights w with a matrix multiply. You'll do some kind of hinge loss to maybe have you compute your loss, you have some regularization term and, and you imagine stitching together all these different operations into some graph structure.

Remember that these graph structures can get pretty complex in the in the case of a big neural net. Now there's many different layers, many different activations, many different weights spread all around in a pretty complex graph.

And as you move to things like neural Turing machines, then you can get these really crazy computational graphs that you can't even really draw because they're so big and messy.

So the the point of deep learning frameworks is really there's really kind of three main reasons why you might want to use one of these deep learning frameworks rather than just writing your own code. So the first would be that these frameworks enable you to easily build and work with these big hairy computational graphs without kind of worrying about a lot of those bookkeeping details yourself.

EM, another major idea is that EM, you, whenever we're working in deep learning, we always need to compute gradients. We're always computing some loss. We're always computing gradient of our weights with respect to the loss, and we'd like to make this this automatically. Computing gradients you don't have, you don't want to have to write that code yourself. You want the framework to handle all these back propagation details for you. So you can just think about writing down the forward pass of your network and have the backward path sort of come out for free without any additional work.

EM, and finally you want all this stuff to run efficiently on gpus so you don't have to worry too much about these low level hardware details about klas and kn n and kuda and moving data between the cpu and gpu memory. You kind of want all those messy details to be taken care of for you. So those are kind of the major, some of the major reasons why you might choose to use frameworks rather than writing your own stuff from scratch.

So as as kind of a concrete example of a computational graph, EM look we, we can maybe write down the super simple thing where we have three inputs inputs X Y and z, we're going to combine X and y to produce a, then we're going to combine a and z to produce b and then finally we're going to do some maybe summing out operation on b to give some scalar final result c, so you've probably written enough numpy code at this point to realize that it's super easy to write down to implement this computational graph or rather's implement this, this bit of computation in numpy, right? Like you can just kind of write down a numpy that you want to generate some random data, you want to multiply two things, you want to add two things, you want to sum out a couple things and it's really easy to do this in d numpy.

EM, but then the question is like suppose that we want to compute the gradient of c with respect to X Y and z, so if you're working in numpy, you kind of need to write out this backward pass yourself. And you've gotten a lot of practice with this on the homeworkworks EM. But it can, it can be kind of a pain and a little bit annoying and messy once you get to really big, complicated things.

EM, the other problem with with numpy is that it doesn't run on the gpu. So numpy is definitely cpu only EM, and you're never going to be able to experience or take advantage of these Gu accelerated speed UPS if you're just stuck working in numpy. And again, it's a pain to have to compute your own gradients in all these situations.

So kind of the, the goal of most deep learning frameworks these days is to let you write code in the forward pass that looks very similar to numpy, but lets you run it on the gpu and lets you automatically compute gradients. And that's kind of the big picture goal of most of these frameworks.

So if you imagine looking at, if we look at an example in tensorflow of the exact same computational graph, EM, we now see that in this forward pass, you write this code that ends up looking very, very similar to the numpy forward pass, where you're kind of doing these multiplication and these addition operations.

But now a tensorflow has this magic line that just computes all the gradients for you, so now you don't have to go in and write your own backward pass. And that's much more convenient.

Um, the other nice thing about tensorflow is you can really just like with one line, you can switch all this computation between cpu and gpu. So here, if you just add this width statement before you're doing this forward pass, you just can explicitly tell the framework, hey, I want to run this code on the cpu.

But now if we just change that width statement a little bit with just a one character change, in this case changing that c to A G, now the code runs on gpu. And now now in this little code snippet, we've solved these two problems, we're running our code on the gpu, and we're having the framework compute all the gradients for us. So that's really nice.

EM and pytorch kind of looks almost exactly the same. So again in pytorch you kind of write down some, you define some variables, EM, you have some forward pass and the forward pass again looks very similar to like in this case, identical to the numpy code. And then again you can just use pytorch to compute gradients all your gradients with with just one line.

And now in pytorch again, it's really easy to switch to gpu. You just need to cast all your stuff to the coa data type before you run your computation. And now everything runs transparently on the gpu for you.

So if you kind of just look at these three examples, like these three snippets of code side by side, the num pi, the tensorflow and the pytorch, you see, you see that like the the tensorflow in the pi torch code and the four pass looks almost exactly like numpy, which is great because numpy has a beautiful API. It's real easy to work with, but we can compute gradients automatically and we could run on gpu automatically.

So after that kind of introduction, I wanted to dive in and talk in a little bit more detail about what's going on inside this tensorflow example.

So EM, as a running example throughout the rest of the lecture I'm going to use the training a two layer fully connected rellud network EM with a with on random data as kind of a running example throughout the rest of the examples here EM and we're going and we're going to train this thing with with an l two euclidean loss on random data. So this is kind of a silly network. It's not really doing anything useful, but it does give you it's relatively small, self contained. The code fits on the slide without being too small and it lets you demonstrate kind of a lot of the useful ideas inside these frameworks.

So here on the right, oh, and then another note, I'm kind of assuming that numpy and tensorflow have already been imported in all these code snippets.

So in, in tensorflow you typically divide your computation into two major stages. First, we're going to write some code that defines our computational graph, and that's that's this red, this red code up in the top half. And then, and then after you define your graph, you're going to run the graph over and over again and actually feed data into the graph to perform whatever computation you want to perform.

EM, so this is really, this is kind of the big common pattern in tensorflow you'll first have a bunch of code that builds the graph and then you'll go and run the graph and reuse it many, many times.

So if you kind of dive into EM the the code of building the graph, in this case up at the top you see that we're defining this X Y W one and w two and we're creating these tf dot placeholder objects. So these, these, these are going to be input nodes to the graph. These are going to be sort of entry points to the graph where when we run the graph we're going to feed in data and put them into these input slots in our, in our computational graph.

So this is not actually like allocating any memory right now. We're just sort of setting up these input slots to the graph.

EM then, now then we're going to use those those input slots which are now kind of like these symbolic variables. And we're going to perform different tensorflow operations on these symbolic variables in order to set up what computation we want to run on those variables. So in this case we're doing a matrix multiplication between X and w, one, we're doing some tf dot maximum to do rayu nonlinearity and then we're doing another matrix multiplication to compute our output predictions. And then we're again using sort of basic tensor operations to compute our euclidean distance, our l two loss between our prediction and the target y.

Another thing to point out here is that these lines of code are not actually computing anything. There's no data in the system right now. We're just building up this computational graph data structure, telling tensorflow which operations we want to eventually run once we put in real data.

So this is just building the graph. This is not actually doing anything.EM then we have this this magical line where after we've comp computed our, our loss in this with these symbolic operations, then we can just ask tensorflow to compute all these, the gradient of the loss with respect to w one and w two in this one magical beautiful line and this avoids you writing all your own backdrop code that you had to do in the assignments.

But again, there's no actual computation happening here. This is just sort of adding extra operations to the, to the computational graph, where now the, the computational graph has these additional operations, which will end up computing these gradients for you.

So now at this point, we've, we've computed our computational graph. We have this big graph in this graph data structure in memory that knows like what operations we want to perform to compute the loss ingredients. And now we enter a tensorflow session to actually run this graph and feed it with data.

So then once we've entered the session, then we actually need to construct some concrete values that will be Fed to the graph. So tensorflow just expects to receive data from numpy arrays in most cases. So here we're just built creating concrete actual values for X Y for X Y W one and w, two using numpy and then storing these in some dictionary.

And now here is where we're actually running the graph. So you can see that we're calling session dot run to actually execute some part of the graph. The first argument loss tells us which part of the graph do we actually want as output EM and that so we actually want the graph in this case. We need to tell it that we actually want to compute loss and grad one and grad w two and we need to pass in with this feed parameter the actual concrete values that will be Fed to the graph.

EM, and then after this, in this one, in this one line, it's going and running the graph and then computing those values for EM for loss grad one, grad w two, and then returning the actual concrete values for those in numpy arrays again. So now after you unpack this output in the second line, you get numpy arrays or you get numpy arrays with the loss the grad and with the loss and the gradients. So then you can go and do whatever you want with these values.

So eh, then it's, it's only so this. This has only run sort of one forward and backward path through our graph and it only takes a couple extra lines if we actually want to train the network. So here we're now. Now we're running the graph many times in a loop. So we're doing a for loop. And in each iteration of the loop we're calling session dot run, asking it to compute the loss and the gradients. And now we're doing a manual gradient descent step using those comp computed gradients to now update our current values of the weights.

So if you actually run this code and plot the losses and you'll see that the loss goes down and the network is training and this is working pretty well. So this is kind of like a super bare bones example of training a fully connected network in tensorflow.

But there's a problem here. So here remember that EM on the forward pass. Every time we execute this graph, we're actually feeding in the weights. We have the weights as numpy arrays and we're explicitly feeding them into the graph. And now when we, when the graph finishes executing, it's going to give us these gradients. And remember the gradients are the same size as the weights. So this means that every time we're running the graph here, we're copying the weights from numpy arrays into tensorflow then getting the gradients and then copying the gradients from tensorflow back out to numpy arrays. So if you're just running on cpu, this is maybe not a huge deal. But remember we talked about c gpu bottleneck and how it's very expensive actually to copy data between cpu memory and gpu memory. So if your network was very large and your weights ingredients were very big, then doing something like this would be super expensive and super slow because we'd be copying all kinds of data back and forth between the cpu and the gpu at every time step. So that's bad. We don't want to do that. We need to fix that.

So EM obviously tensorflow has some solution to this. And the idea is that now we want our weights w one and w two rather than being place holders where we're going, where we expect to feed them into the network and every forward path. Instead, we define them as variables. So a variable is something that is a value that lives inside the computational graph, and it's going to persist inside the computational graph across different times when you run the same graph.

So now instead of declaring these w one and w two as as placeholders instead we just construct them as variables. But now since they live inside the graph, we also need to tell tensorflow how they should be initialized, right? Because in the previous case we were feeding in their values from outside the graph, so we initialize them in numpy. But now because these, because these things live inside the graph, tensorflow is responsible for initializing them. So we need to pass in a tf dot random normal operation, which again is not actually initializing them when we run this line. This is just telling tensorflow how we want them to be initialized. So it's a bit a little bit of confusing misd direction going on here.

And now, now remember in the previous example we were, we were actually updating the weights outside of the computational graph we were using. In the previous example, we were computing the grad and then using them to update the weights as numpy arrays and then feeding in the updated weights at the next timest step. But now, because we want these weights to live inside the graph, this operation of updating the weights needs to also be an operation inside the computational graph.

So now we use this assign function, which mutates these variables inside the computational graph, and now the mutated value will persist across multiple runs of the same graph.

So now when we run, when we run this graph and when we train the network, now we need a special. We need to run the graph once with a bit of special incantation to tell tensorflow to set up these variables that are going to live inside the graph. And then once we've done that initialization, now we can run the graph over and over again. And here now we're only feeding in the data and the data and labels X and y, and the weights are living inside the graph. And here we've asked the network to. We've asked tensorflow to compute the loss for us and, and then you might think that this would train the network.

But there's actually a bug here.So if you actually run this code and you plot the loss, it doesn't train.

So that's bad. It's confusing. Like what's going on? We wrote this assign code, we ran the thing like we computed the loss in the gradients and our loss is flat. What's going on?

Any ideas?Yeah, so, so one one hypothesis was that maybe we're accidentally re-initializing the w's every time we call the graph. That's a good hypothesis. That's actually not the problem in this case.

So the, the, the answer is that we actually need to explicitly tell tensorflow that we want to run these new w one and new w two operations. So we've built up this big computational graph data structure in memory. And now when we call run, we only told tensorflow that we wanted to compute loss. And if you kind of look at the dependencies among these different operations inside the graph, you see that in order to compute loss, we don't actually need to perform this update operation. So tensorflow is smart and it only computes the parts of the graph that are necessary for computing the output that you asked it to compute.

So that's kind of a nice thing because it means it's only doing as much work as it needs to. But in situations like this it can be a little bit confusing and lead to behavior that you didn't expect EM. So the solution in this case is that we actually need to explicitly tell tensorflow to perform those update operations. So one thing we could do, which is what was suggested, is we could add new w one and new w two as outputs and just tell tensorflow that we want to produce these values as outputs. But that's a problem too, because the values, those new w one new w two values are again these big gpus or these big tensors. So now if we tell tensorflow we want those as output, we're going to again get this copying behavior between cpu and gpu at every iteration. So that's bad. We don't want that.

So there's a little trick you can do instead, which is that we add kind of a dummy node to the graph with these fake data dependencies. And we just say that this dummy node updates has these data dependencies of new w one and new w two, and now when we actually run the graph, we tell it to compute both the loss and this dummy node. And this dummy node doesn't actually return any value, it just returns none. But because of this dependency that we've put into the into the node, it ensures that when we run the updates value, we actually also run these update operations. So question.

Is there a reason that we didn't put X and y into the graph and that stayed that it stayed as numpy so EM, in this example we we're reusing X and y in every were reusing the same X and y and every iteration. So you're right, we could have just also stuck those in the graph. But in in a more realistic scenario, X and y will be mini, mini batches of data. So those will actually change at every iteration and we will want to feed different values for those at every iteration. So in this case they could have stayed in the graph but in most cases they will change. So we don't want them to live in the graph.

So now this is another question.Yeah, so we, we told it we, we we put into tensorflow that the outputs we want are loss and updates EM updates. EM is just is just like it, not actually a real value. So when updates evaluates it just returns none. But because of this dependency we've told it that updates depends on on these assign operations. But these assign operations live inside the computational graph and all live inside gpu memory. So then we're doing these these update operations entirely on the gpu and we're no longer copying the updated values back outside back out of the graph.

So the the question is does T F dot group return none? So this gets into the trickiness of tensorflow. So T F dot group returns some crazy tensorflow value. EM, it doesn't. Actually, it sort of returns some like internal tensorflow node operation that we need to continue building the graph. But when you execute the graph and when you tell inside the session dot run, when we told it we wanted to compute the concrete value from updates, then that returns none.

So whenever you're working with tensorflow, you have this funny indirection between building the graph and then the actual output values during building. The graph is some funny, weird object. And then you actually get a concrete value when you run the graph. So here, after you run updates, then the output is none.

Does that clear it up a little bit?As why is loss a value and why is updates none, that's just the way that updates works. So loss is a value when we compute. When we tell tensorflow we want to run a tensor, then we get the concrete value updates. Is this kind of special other data type that does not return a value? It instead returns none. So it's, it's kind of some tensorflow magic that's going on there. Maybe we can talk offline if you're still confused.

Yeah, yeah, that behavior is coming from the group method.EM so now we we kind of had this weird pattern where we wanted to do these different assign operations. We have to use this funny tf dot group thing that's kind of a pain. So EM thankfully tensorflow gives you some convenience operations that kind of do that kind of stuff for you EM and that's called an optimizer. So here EM we're using tf dot train dot gradient descent optimizer and we're telling what learning rate, what learning rate we want to use and you can imagine that there's add on, there's rms prop, there's all kinds of different optimization algorithms here and now we call optimizer optimizer dot minimize of loss and now this this is a pretty magical. This is a pretty magical thing because now this this call is aware that these variables w one and w two are marked as trainable by default. So then internally inside this optimizer dot minimize it's going in and adding nodes to the graph which will compute gradient of loss with respect to w one and w two and then it's also performing that update operation for you and it doing the grouping operation for you and it's doing the assigns. It's like doing a lot of magical stuff inside there.

But then it ends up giving you this magical updates value, which if you dig through the code, they're actually using tf dot group. And so it looks very similar internally to what we saw before. And now when we run the graph inside our loop, we do the same pattern of telling it to compute loss and updates. And every time we call, every time we tell the graph to compute updates, then it'll actually go and update the graph.

Question.What is the T F dot global variable variables initializer so that's initializing w one and w two because these are variables which live inside the graph. So we need to, we saw this when we create the tf dot variable, we have this tf dot random normal, which is this initialization. So EM the tf dot global variables initializer is causing the tf dot random normal to actually run and generate concrete values to initialize those variables.

So it so it knows that a placeholder is going to be Fed outside the graph and a variable is something that lives inside the graph. So I don't know all the details about how it decide what exactly it decides to run. With that call, I think you'd have to dig through the code to figure that out or maybe it's documented somewhere.

EM. So, but now we've kind of got this again. We've got this fully, this full example of training a network in tensorflow and we're kind of adding bells and whistles to make it a little bit more convenient.

So we can also, here in the previous example, we were computing the loss explicitly using our own tensor operations tensorflow like you can always do that. You can use basic tensor operations to compute just about anything you want, but tensorflow also gives you a bunch of sort of convenience functions that compute these common neural network things for you. So in this case, we can use tf dot losses dot mean squared error and it just does the l two loss for us. So we don't have to compute itself in terms of basic tensor operations.

So another, another kind of weirdness here is that it was kind of annoying to that we had to explicitly define our inputs and define our weights and then chain them together in the forward pass using a matrix multiply and, and in this example we've actually not put biases in the layer because that would be kind of an extra. Then we'd have to initialize biases, we'd have to get them in the right shape. We'd have to broadcast the biases against the output of the matrix multiply and you could see that that would kind of be a lot of code. It would be kind of annoying to write. And once you get to like convolutions and bash normalizations and other types of layers, EM, this this kind of basic way of working of like having these variables, having these inputs and outputs and combining them all together with basic computational graph operations could be a little bit unwieldy. And it could be really annoying to make sure you initialize the weights with the right shapes and all that sort of stuff.

So as a result, there's a bunch of sort of higher level libraries that wrap around tensorflow and handle some of these details for you. So one example that ships with tensorflow is this tf dot layers inside. So now in this code example, you can see that our code is only explicitly declaring the X and the y, which are the place holders for the data and the labels. And now we say that.

H equals tf dot layers dot dense. We give it the input X and we tell it units equals h and now this will, this is again kind of a magical line because inside this line it's kind of setting up w one and and and b one the bias. It's setting up variables for those with the right shapes that are kind of inside the graph but a little bit hidden from us. And it's using this Xavier initializer object to set up an initialization strategy for those. So before we were doing that explicitly ourselves with this tf dot random normal business, but now here it's kind of handling some of those details for us and it's just spitting out an h, which is again the same sort of h that we saw in the previous layer is just doing some of those details for us.

And you can see here we're also passing an activation equals tf dot nn dot relu. So it's even doing the activation, the relu activation function inside this layer for us. So it's taking care a lot of a lot of these architectural details for us. EM question.

Does the Xavier initialize your default to particular distribution? I'm sure it has some default. I'm not sure what it is. I think you'll have to look at the documentation, but it seems to be a reasonable strategy, I guess. And in fact, if you run this code, it converges much faster than the previous one because the initialization is better.

EM and you can see that we're using two calls to tf dot layers and this lets us build our model without doing all these explicit book bookkeeping detailsself so this is maybe a little bit more convenient EM, but tf dot contrib dot layer is really not the only game in town. There's like a lot of different higher level libraries that people build on top of tensorflow and it's, it's kind of due to this basic impotence mismatch where the computational graph is a relatively low level thing, but when we're working with neural networks we have this concept of layers and weights and some layers have weights associated with them and, and we typically think at a slightly higher level of abstraction than this raw computational graph. Um, so that's what these, these various packages are trying to, eh, help you out and let you work at this higher layer of abstraction.

So another very popular package that you may have seen before is keras keras is a very beautiful, nice API that sits on top of tensorflow and handles sort of building up these computational graphs for you up in the back end.

So EM, by the way, keras also supports the as a back end. So eh, that's, that's also kind of nice. And in this example you can see we build the model as a sequence of layers. EM, we build some optimizer object and we call model dot compile. And this does a lot of magic in the back end to build the graph.

And now we can call model dot fit, and that does the whole training procedure for us magically. So I don't know all the details of how this works, but I know keras is very popular, so you might consider using it if you're talking about tensorflow.

Question.Yeah, so the question is like why there's no explicit c pu, g pu going on here. So I've kind of left that out to keep the code clean. But you saw at the beginning, at the beginning examples, it was pretty easy to flop all these things between cpu and gpu and there's either some global flag or some different data type or some width statement, and it's usually relatively simple and just about one line to swap in each case. But exactly what that line looks like differs a bit exactly depending on the situation.

EM, so there's actually like this whole large set of, of higher level tensorflow wrappers that you might see out there in the wild EM. And it seems that like even people within Google can't really agree on which one is the right way to use, but which one is the right one to use. So keras and tf learn are third party libraries that are out there on the Internet by other people. Um, but there's these three different ones, tf dot layers, tf slim and tf dot contrib dot learn that all ship with tensorflow that are all kind of doing this, a slightly different version of this higher level wrapper thing.

There's another framework also from Google, but not shipping with tensorflow called pretty tensor that does the same sort of thing. And I guess none of these were good enough for deepmind because they went ahead a couple weeks ago and wrote and released their very own high level tensorflow wrapper called sonnet. So I I wouldn't, I wouldn't be grudge you if you were kind of confused by all these things. There's a lot of different choices. They don't always play nicely with each other.

But you have a lot of options. That's good.EM tensorflow has pre train models. EM, there's some examples in tf, slim EM and in keras because remember pre train models are super important when you're training your own things. EM, there's also this idea of tensor board where you can load up your, I don't want to get into details but tensor board, you can add sort of instrumentation to your code and then plot losses and things as you go through the training process.

EM tensorflow also lets you run distributed, where you can break up a computational graph run on different machines. That's super cool, but I think probably not really anyone outside of Google is really using that to great success these days. But if you do want to run distributed stuff, probably tensorflow is the main game in town for that.

EM, a side note is that a lot of a lot of the design of tensorflow is kind of spiritually inspired by this earlier framework called the from Montreal. EM, I don't want to go through the details here. Just if you go through these slides on your own, you can see that the code for the ends up looking very similar to tensorflow where we define some variables, we do some forward paths, we compute some gradients and we compile some function. Then we run the function over and over to train the network. So it kind of looks a lot like tensorflow.

So EM, we still have a lot to get through, so I'm going to move on to pytorch and maybe take questions at the end. EM, so pytorch from Facebook is kind of different from tensorflow in that we have sort of three explicit different layers of abstraction inside pytorch. So pytorch has this, ah, this tensor object which is just like a numpy array. It runs imperatively. Um, it's just an imperative array. It doesn't know anything about deep learning, but it can run the gpu. We have this variable object which is a node, and a computational graph which builds up computational graph, lets you compute gradients, that sort of thing. And we have a module object which is a neural network layer that you can compose together these modules to build big, big networks.

So if you kind of want to think about rough equivalents between pytorch and tensorflow you can think of the the pytorch tensor as fulfilling the same role as the num py array and tensorflow the pytorch variable is similar to the tensorflow tensor or variable or placeholder which are all sort of nodes in a computational graph. And now the the pytorch module is kind of equivalent to these these higher level things from tf dot slim or tf dot layers or sonnet or these other higher level frameworks. So right away one thing to notice about pytorch is that um, because it ships with this higher level abstraction and like one really nice higher level abstraction called modules on its own, there's sort of less choice involved. Just stick with stick with the n modules and you'll be good to go. You don't need to worry about which higher level, which higher level wrapper to use.

So EM pytorch tensors as I as I said are just are just like numpy arrays. So one thing. So here on the right we've done an entire two layer network using entirely pytorch tensors. One thing to note is that we're not importing numpy here at all anymore. We're just doing all these operations using pytorch tensors EM. And this, this, this code looks exactly like sort of the two layer net code that you wrote in numpy on the first homework. So you you set up some random data, EM, you use some operations to compute the forward pass and then we're explicitly doing the backward passself just sort of backropping through the network through the operations, just as you did on homework one.

And now we're doing a manual update of the manual update of the weights, using our learning rate and using our computed gradients.

EM and it's really but the major difference between the pytorch tensor and numpy arrays is that they run on gpu. So EM, all you have to do to make this code run on gpu is use a different data type rather than using torch dot float tensor you do torch dot coa dot float tensor EM cast all of your all of your tensors to this new data type and everything runs magically on the gpu. So you you should think of pyt torch tensors as just num pi plus gpu that that's exactly what it is, nothing of nothing specific to deep learning.

So the next layer of abstraction in pytorch is the variable. So this is once we move from tensors to variables. Now we're building computational graphs and we're able to take gradients automatically and everything like that EM. So here, if X is a variable, then X dot data is a tensor. And X dot grad is another variable containing the gradients of the loss with respect to that tensor. So X dot grad dot data is an actual tensor containing those gradients.

EM and pytorch tensors and variables have the exact same API, so any code that worked on pytorch tensors, you can just make them variables instead and run the same code, except now you're building up a computational graph rather than just doing these imperative operations.

EM, so here we've done the exact same. So here when we create these variables, each call, each call to the variable constructor wraps a py torch tensor and then also gives a flag whether or not we want to compute gradients with respect to this variable.

And now, in the forward pass, it looks exactly like it did before in the variable in the, in the case with tensors, because they have the same API. So now we're computing our predictions, we're computing our loss in kind of this imperative kind of way.

EM, and then we need to and then we call loss dot backwards. And now all these gradients come out for us and then we can make a gradient update step on our weights using the gradients that are now present in the w one dot grad dot data. So this ends up looking. This ends up looking quite like the numpy case except all the gradients come for free.

One thing to note that's kind of different between pytorch and tensorflow is that in the tensorflow case, we were building up this explicit graph, then running the graph many times here in pytorch. Instead, we're building up a new graph every time we do a forward pass, and this makes the code look a bit cleaner, and it has some other implications that we'll get to in a bit.

So in pytorch EM, you can define your own new aut grad functions by defining the forward and backward in terms of tensors. EM, this ends up looking kind of like the module layers code that you write for homework two, where you can implement forward and backward using tensor operations and then stick these things inside computational graphs. So here we're defining our own relu and then we can actually go and and use our own use our our own relu operation and now stick it inside our computational graph and define our own operations this way.

EM, but most of the time you will probably not need to define your own autgrad operations. Most of the times, the operations you need will mostly be already implemented for you.

So in in, in tensorflow we we saw we have if we can move to something like keras or tf dot learn, and this gives us a higher level API to work with rather than this raw computational graphs. The equivalent in pytorch is the nn package where it provides these high level high level wrappers for working with these things. So, but unlike tensorflow, there's only one of them and it works pretty well. So just use that if you're using pytorch.

So here this ends up looking kind of like keras, where we define our model as some sequence of layers of linear and relu operations, and we use some loss function defined in the nn package. That's our mean squared error loss.

And now, inside each iteration of our loop, we can run data forward through the model to get our predictions. We can run the predictions for through the loss function to get our scalar loss. Then we can call loss backward, get all our gradients for free, and then loop over the parameters of the models and do our explicit gradient descent step to update the models. And again, we see that we're sort of building up this new computational graph every time we do a forward pass.

EM, and just like we saw in tensorflow pytorch provides these optimizer operations that kind of abstract away this updating logic and implement fancier update rules like atom and whatnot. So here we're constructing an optimizer object, telling it that we want it to optimize over the parameters of the model, giving it some learning rate on other hyper parameters.

And now, after we compute our gradients, we can just call optimizer step and it updates all the parameters of the model for us right here.

EM so another common thing you'll do in pytorch a lot is actually define your own nn modules. So typically you'll write your own class, which defines your entire model as a single new and end module class. And a module is just kind of a neural network layer that can contain either other other modules or trainable weights or other other kinds of state.

So in this case, we can redo the two layer net example by defining our own and end module class. So now here in the initializer of the class, we're assigning this linear one and linear two, we're constructing these new module objects and then storing them inside inside of our own class.

And now in the forward pass we can use both our own internal modules as well as arbitrary autograph operations on variables to compute the output of our network. So here we receive the inside inside this forward method. Here the input acts as a variable and then we pass the variable to our self dot linear one for the first layer. We use an autgrad op clamp to compute the relu. We pass the output of that to the second linear and then that gives us our output.

And now, eh, now the rest of this code for training, this thing looks pretty much the same where we build an optimizer and loop over and on every iteration feed data to the model, compute the gradients with loss backwards called optimizer dot step. So this, this is like relatively characteristic of what you might see in a lot of pytorch type training scenarios where you define your own class, defining your model that contains other modules and whatnot, and then you have some explicit training loop like this that runs it and updates it.

EM one kind of nice quality of life thing that you have in pytorch is a data loader. So a data loader can handle building mini batches for you. It can handle some of the multi threading that we talked about for you where it can actually use multiple threads in the background to build mini batches for you and stream off disk.

EM, so this is so here. A data loader wraps a data set and provides some of these abstractions for you. And in practice, when you want to run your own data, you typically will write your own data set class, which knows how to read your particular type of data off whatever source you want, and then wrap it in a data loader and train with that.

EM, so here we can see that now we're iterating over the over the data loader object. And in every iteration this is yielding mini batches of data and it's internally handling the shuffling of the data and multi threaded data loading and all this sort of stuff for you. So this is kind of a complete pytorch example and a lot of pytorch training code ends up looking something like this.

EM pytorch provides pre trained models and this is probably the slickest pre trained models experience I've ever seen. You just say torch vision dot model dot alexnet pre trained equals true. That'll go down in the background, download the pre trained weights for you if you don't already have them and then it's right there. You're good to go. So this is super easy to use.

EM pytorch also has, there's also a package called wisdom that lets you visualize some of these law statistics, somewhat similar to tensor board EM, but that so that's kind of nice. I haven't actually gotten a chance to play around with this myself, so I can't really speak to how useful it is. But one of the major differences between tensor board and wisdom is that tensor board actually lets you visualize the structure of the computational graph, which is really cool, a really useful debugging strategy, and vizom does not have that functionality yet.

But I've never really used this myself so I can't really speak to its utility.EM, as a bit of an aside, pyt torch is kind of an evolution of kind of a newer updated version of an older framework called torch, which I worked with a lot in the last couple of years, EM, and I don't want to agree through the details here, but eh, pytorch is pretty much better in a lot of ways than than the old lua torch, but they actually share a lot of the same back end c code for computing with tensors and gpu operations on tensors and whatnot.

So if you look through this torch example, some of it ends up looking kind of similar to py torch. Some of it's a bit different. Maybe you can step through this offline.

EM but kind of the the high level differences between torch and pyt torch are that torch is actually in lua, EM not python unlike these other things. So learning lua is a bit of a turn off for some people eh torch is doesn't have autgrad torch is also older so it's more stable, less susceptible to bugs. There's maybe more example code for torch EM they're about the same speeds that's not really a concern but EM in pytorch it's in python which is great. You've got autad which makes it a lot simpler to write complex models. EM in lua torch you end up writing a lot of your own backrop code. Sometimes that's a little bit annoying EM but pytorch is newer, there's less existing code EM it's still subject to change EM so it's a little bit more of an adventure.

EM, but at least for me, I I kind of prefer, I don't really see much reason for myself to use torch over pytorch anymore at this time, so I'm I'm pretty much using pytorch exclusively for all my work these days.

EM, we talked about this a little bit about this idea of static versus dynamic graphs, and this is one of the main distinguishing features between pytorch and tensorflow. So we saw in tensorflow you have, you have this two stages of operation where first you build up this computational graph. Then you run the computational graph over and over again, many, many times, reusing that same graph. That's called a static computational graph because there's only one of them.

And we saw pytorch was quite different, where we're actually building up this new computational graph, this new fresh thing on every forward pass that's called a dynamic computational graph for kind of simple cases with kind of feed forward neural networks. It doesn't really make a huge difference. The code ends up looking kind of similarly and they work kind of similarly. But I do want to talk a bit about some of the implications of static versus dynamic and what are the trade offs of those two.

Um.So one kind of nice idea with static graphs is that because we're kind of building up one computational graph once and then reusing it many times, the framework might have the opportunity to go in and do optimizations on that graph and kind of fuse some operations, reorder some operations, figure out the most efficient way to operate that graph. So it can be really efficient. And because we're going to reuse that graph many times, maybe we can. Maybe that optimization process is expensive upfront, but we can amortize that cost with the speedups that we've gotten when we run the graph many, many times.

So as kind of a concrete example, EM, maybe if you write some graph which has convolution and rel operations kind of one after another, you might imagine that some, some fancy graph optimizer could go in and actually output like emit custom code which has fused operations, fusing the convolution and the relu. So now it's, it's computing. The same thing that you did that you as the code you wrote but now might be, might be able to be executed much more efficiently.

EM, so I'm not too sure on exactly what the state in in practice of of tensorflow graph optimizations is right now, but at least in principle, this is one, this is one place where a static graph really you can do this. You can have the potential for doing this optimization in static graphs where maybe would be not so tractable for dynamic graphs.

Another kind of subtle point about static versus dynamic is this idea of serialization. So with a static graph, you can imagine that you write this code that builds up the graph and then once you've built the graph, you have this data structure in memory that represents the entire structure of your network. And now you could take that data structure and just serialize it to disk. And now you've got the whole structure of your network saved in some file and then you could later reload that thing and then run run that computational graph without access to the original code that built it. So this would be kind of nice in a deployment scenario. You might imagine that you might want to train your network in python because it's maybe easier to work with, but then after you serialize that network and then you could deploy it, now it may be A C plus plus environment where you don't need to use the original code that built the graph. So that's kind of a nice advantage of static graphs.

Whereas with a dynamic graph, because we're interleaving these processes of graph building and graph execution, you kind of need the original code at all times if you want to reuse that model in the future.

On the other hand, some advantages for dynamic graphs are that it kind of makes, it just makes your code a lot cleaner and a lot easier in a lot of scenarios. So for example, suppose that we wanted to do some some conditional operation where depending on the value of some variable z, we want to do different operations to compute y, where if z is positive, we want to use one weight matrix. If z is negative, we want to use a different weight matrix and we just want to switch off between these two alternatives.

EM, in pytorch because we're using dynamic graphs, it's super simple. Your code kind of looks exactly like you would expect, like exactly what you would do in numpy. You can just use normal python control flow to handle this thing. And now because we're building up the graph each time, each time we perform this operation, we'll take one of the two paths and build up maybe a different graph on each forward paths. But for any graph that we do end up building up, we can back propagate through it just fine. And the code is very clean, easy to work with.

EM, now in tensorflow the situation is a little bit more complicated because we want to because we build the graph once this control flow operator kind of needs to be an explicit operator in the tensorflow graph EM. And now so then you can see that we have this tf dot com to call, which is kind of like a tensorflow version of an if statement, but now it's baked into the computational graph rather than using sort of python control flow. And the problem is that because we only build the graph once, all the potential paths of control flow that our program might flow through need to be baked into the graph at the time we construct it before we ever run it. So that means that any kind of control flow operators that you want to have need to be not python control flow operators. There you need to use some kind of magic special tensorflow operations to do control flow. In this case it tf dot com.

EM another, another kind of similar situation happens if you want to have loops. So, suppose that we want to compute some kind of recurrent relationship where maybe Y T is equal to Y T minus one plus X T times some weight matrix w, and then depending on each time we do this, every time we compute this, we might have a different size sequence of data and no matter the length of our sequence of data, we just want to compute this same recurrence relation no matter the size of the input sequence.

So EM in in pytorch this is super easy. We can just kind of use a normal, a normal for loop in python to just loop over the number of times that we want to unroll. And now depending on the size of the input data, our computational graph will end up as different sizes. But that's fine. We can just back propagate through each one one at a time.

EM, now in tensorflow this becomes a little bit uglier EM and again because because we need to construct the graph all at once up front, this control flow looping construct again needs to be an explicit node in the tensorflow graph. So I hope you remember your functional programming because you'll have to use those kinds of operators to implement looping constructs in tensorflow. So in this case, for this particular recurrence relationship, EM, you can use a fold l operation and pass and pass in sort of implement this particular loop in terms of a fold l.

EM, but what this basically means is that you have this sense that tensorflow is almost building its own entire programming language in using the language of computational graphs and any kind of control flow operator. Any kind of data structure needs to be rolled into the computational graph. So you, you can't really utilize all your favorite paradigms for working imperatively in python, you kind of need to relearn a whole separate set of control flow operators. And if you want to do any kinds of control, flow inside your computational graphs using tensorflow.

EM, so at least for me, so at least for me, I find that kind of confusing, a little bit hard to wrap my head around sometimes. And I kind of like that using pytorch dynamic graphs, you can just use your favorite imperative programming constructs and it all works just fine.

EM by the way, EM, there actually is some some nice some very new library called tensorflow fold which lets you, which is kind of layer another one of these layers on top of tensorflow that lets you implement dynamic graphs. EM, you kind of write your own code using tensorflow fold that looks kind of like a dynamic graph operation and then tensorflow full does some magic for you and and somehow implements that in terms of these static tensorflow graphs. EM, this is a super new paper that's being presented at I clear this week in France. EM, so I haven't, I haven't had the chance to dive in and and play with this yet, but my initial impression was that it does add some amount of dynamic graphs to tensorflow but it is still a bit more awkward to work with than those sort of native dynamic graphs you have in pytorch.

So um then I think I thought it might be nice to motivate like why, why would we care about dynamic graphs in general? So one one one option is recurrent networks. So you can see that for something like image captioning eh we use a recurrent network which which operates over sequences of different lengths. In this case eh the the sentence that we want to generate as a caption is a sequence and that sequence can vary depending on our input data EM. So now you can see that that we have this dynamism in in the, in the in the thing where depending on the size of the sentence our computational graph might need to have more or fewer elements. So that's one kind of common application of dynamic graphs.

EM, for for those of you who took CS two, two, four n last quarter, EM, you saw this idea of recursive networks where sometimes in natural language processing you might for example, compute a parse tree of a sentence and then you want to have a neural network kind of operate recursively up this parse tree. So having a neural network that kind of works, not just, it's not just a sequential sequence of layers, but instead it's kind of working over some graph or tree structure instead. Where now each data point might have a different graph or tree structure. So the structure of the computational graph then kind of mirrors the structure of the input data and it could vary from data point to data point.

EM, so this type of thing seems kind of complicated and hairy to implement using tensorflow EM, but in py torch you can just kind of use like normal python control flow and it'll work out just fine.

Another bit of more research e application EM, is this really cool idea that I like called neural module networks for visual question answering? So here the idea is that we want to ask some questions about images where we maybe input this image of cats and dogs. There's some question what color is the cat? And then the internally the system can read the question and then has these different specialized neural network modules for performing operations like asking for colors and finding cats. And then depending on the text of the question, it can compile this custom architecture for answering the question.

EM. And now if we have, if we ask a different question, like, are there more cats than dogs? Now we have maybe the same basic set of modules for doing things like finding cats and dogs and counting, but they're arranged in a different order. So we get this dynamism again where different data points might give rise to different computational graphs, but this is a bit more of a research e thing and maybe not so mainstream right now.

But as kind of a bigger point, I think that there's a lot of cool creative applications that people could do with dynamic computational graphs, and maybe there aren't so many right now just because it's been so painful to work with them. So I think that there's a lot of opportunity for doing cool, creative things with dynamic computational graphs, and maybe if you come up with cool ideas, we'll feature it in lecture next year.

Uh, so I wanted to talk very briefly about cafe, which is this frame from Berkeley. EM, which cafe is somewhat different from the other deep learning frameworks where you in many cases you can actually train networks without writing any code yourself. You kind of just call into these pree existing binaries, set up some configuration files, and in many cases you can train on data without writing any of your own code.

So you maybe first you convert your data using into some format like H D F, five or lmdb and there exist some scripts inside cafe. They can just convert like folders of images and text files into these formats for you.

EM, you need to define now instead of writing code to define the structure of your computational graph, instead you edit some text file called a proto txt, which sets up the structure of the computational graph EM. Here the structure is that we read from some input hdfi file. We perform some inner product, we compute some loss, and the whole structure of the graph is set up in this text file.

EM one kind of downside here is that these these files can get really ugly for very large networks. So for something like the hundred and fifty two layer resnet model, which by the way was trained in cafe originally, then this proto txt file ends up almost 7000 lines long. So people are not writing these by hand. You people sometimes will like write python scripts to generate these proto txt files and then you're kind of in the realm of rolling your own computational graph abstraction. That's probably not a good idea, but I've seen that before.

EM then, rather than having some optimizer object, instead there's some solver. You define some solver things inside another. Proto txt. This defines your learning rate, your optimization algorithm and whatnot. And then once you do, once you do all these things, you can just run the cafe binary with the train command and it all happens magically.

EM cafe has a model zoo with a bunch of pre trained models. That's, that's pretty useful. Cafe has a python interface, but it's not super well documented. You kind of need to read the source code of the python interface to see what it can do. That's kind of annoying, but it does work.

EM, so kind of my my general output. My general thing about cafe is that it's maybe good for feed forward models. It's maybe good for production scenarios because you don't. It doesn't depend on python but probably for research these days I've seen cafe being used maybe a little bit less, although I think it is still pretty commonly used in industry again for production.

EM I promise one slide on one or two slides on cafe two, so cafe two is this successor to cafe which is from Facebook. EM, it's super new. It was only released a week ago so I I really haven't had the time to form a super educated opinion about cafe two yet EM. But it uses static graphs kind of similar to tensorflow EM kind of like cafe one, the core is written in c plus plus and they have some python interface. The difference is that now you no longer need to write your own python scripts to generate protototy t files. You can kind of define your your computational graph structure all in python kind of looking with an API that looks kind of like tensorflow but then you can spit out like you can serialize this computational graph structure to a proto txtd file and then once your model is trained and whatnot then we get this benefit that we talked about of static graphs where you can, you don't need the original training code now in order to deploy a train model.

EM, so one interesting thing is that we've seen Google maybe has one major deep learning framework which is tensorflow where Facebook has these two, both py pytorch and cafe two, so these are kind of different philosophies. EM, Google is kind of trying to build one framework to rule them all. That maybe works for every possible scenario for deep learning. This is kind of nice because it consolidates all efforts onto one framework. It means you only need to learn one thing and it will work across many different scenarios including like distributed systems, production deployment, mobile research, everything only need to learn one framework to do all these things.

Whereas Facebook is taking a bit of a different approach where pytorch is really more specialized, more geared towards research. So in terms of writing research code and quickly iterating on your ideas, that's super easy in pytorch but for things like running in production, running on mobile devices, pytorch doesn't have a lot of great support. Instead, cafe two is kind of geared towards those more production oriented use cases.

So my my kind of general study. My, my general overall advice about like which framework to use for which problems is is kind of that both I think both. I think tensorflow is a pretty safe bet for just about any project that you want to start new, right because it is sort of one framework to rule them all. It can be used for just about any circumstance. EM however you probably need to pair it with a higher level wrapper and if you want dynamic graphs you're maybe out of luck. EM, some of the code ends up a little bit looking a little bit uglier in my in my opinion but maybe that's kind of a cosmetic detail and it doesn't really matter that much EM. I I personally think pytorch is really great for research if you're focused on just writing research code. I think pytorch is a great is a great choice but it's a bit newer. It has less community support, less code out there so it could be a bit of an adventure. If you want more of a well trodden path, tensorflow might be a better choice.

Um, if you're interested in production deployment, you should probably look at cafe, cafe two or tensorflow. And if you're really focused on mobile deployment, I think tensorflow and cafe two both have some built in support for that. So it's kind of unfortunate. There's not just like one global best framework. It kind of depends on what you're actually trying to do, what applications you anticipate. But these are kind of my general advice on those things.

So next time, we'll talk about some case studies about various CNN architectures.